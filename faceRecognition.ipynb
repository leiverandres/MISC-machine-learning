{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "faceRecognition.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leiverandres/MISC-machine-learning/blob/master/faceRecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i06iBE0n7Cwr",
        "colab_type": "text"
      },
      "source": [
        "# Assignment #3: Face recognition task\n",
        "\n",
        "Leiver Andres Campeón  -  Juan Sebatián Vega\n",
        "\n",
        "The goal of this assigment is to compare two machine learning algorithms and understand the importants that dimensionality reduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq_Kl-Jr7Kot",
        "colab_type": "code",
        "outputId": "a66ea96a-5bb2-420d-eba1-15132cbaa8f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 575
        }
      },
      "source": [
        "!pip3 install tensorflow==2.0.0-beta1;\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.datasets import fetch_lfw_people\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import learning_curve\n",
        "import tensorflow as tf\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-beta1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/6c/2c9a5c4d095c63c2fb37d20def0e4f92685f7aee9243d6aae25862694fd1/tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (87.9MB)\n",
            "\u001b[K     |████████████████████████████████| 87.9MB 1.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (3.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.16.4)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.1.7)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603 (from tensorflow==2.0.0-beta1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 34.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 (from tensorflow==2.0.0-beta1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 47.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.15.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.33.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-beta1) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (0.15.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.14.0\n",
            "    Uninstalling tensorflow-1.14.0:\n",
            "      Successfully uninstalled tensorflow-1.14.0\n",
            "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj01b8f5pEQ1",
        "colab_type": "text"
      },
      "source": [
        "## Overall dataset acquisition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ULSkEHCS6pJI",
        "colab_type": "code",
        "outputId": "7d67ce78-8f66-48c1-d835-3a4cf7e32ddd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "lfw_people = fetch_lfw_people(min_faces_per_person=100, resize=0.4, )\n",
        "data = lfw_people.data\n",
        "target = lfw_people.target\n",
        "\n",
        "plt.matshow(lfw_people.images[1])\n",
        "plt.show()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAFQCAYAAABZFwDrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nPS0HkCRMwAAAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 288x389.189 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBLM17K8-GlO",
        "colab_type": "code",
        "outputId": "358a828c-b800-44a5-8173-a6818b8491cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "k_classes = len(lfw_people.target_names)\n",
        "examples, height, width = lfw_people.images.shape \n",
        "features = height * width\n",
        "\n",
        "X = lfw_people.images\n",
        "y = lfw_people.target\n",
        "\n",
        "print(f\"This dataset contains {examples} examples\")\n",
        "print(f\"For {k_classes} classes, each image is {height}x{width}\")\n",
        "print(f\"Features: {features}\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This dataset contains 1140 examples\n",
            "For 5 classes, each image is 50x37\n",
            "Features: 1850\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N8reYqDYCDkb",
        "colab_type": "code",
        "outputId": "0e27e45f-0f19-4ff7-87e0-d5fa24e9b388",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "class2idx = {name: idx for (idx, name) in enumerate(lfw_people.target_names)}\n",
        "idx2class = {idx: class_name for class_name, idx in class2idx.items()}\n",
        "print(class2idx)\n",
        "print(idx2class)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Colin Powell': 0, 'Donald Rumsfeld': 1, 'George W Bush': 2, 'Gerhard Schroeder': 3, 'Tony Blair': 4}\n",
            "{0: 'Colin Powell', 1: 'Donald Rumsfeld', 2: 'George W Bush', 3: 'Gerhard Schroeder', 4: 'Tony Blair'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFGd1tmJs4iT",
        "colab_type": "text"
      },
      "source": [
        "### Get train, val, and test splits\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5IRYVhbBf4D",
        "colab_type": "code",
        "outputId": "29cba307-2ff6-4393-d40f-a166c893877d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "source": [
        "# Get train, val, test splits\n",
        "\n",
        "X_train, X_tmp, y_train, y_tmp = train_test_split(X, y, test_size=0.3)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5)\n",
        "print(f'Train split(70%): {len(y_train)}')\n",
        "print(f'Validation split(15%): {len(y_val)}')\n",
        "print(f'Test split(15%): {len(y_test)}')"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train split(70%): 798\n",
            "Validation split(15%): 171\n",
            "Test split(15%): 171\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cPFuNeJ61iD",
        "colab_type": "text"
      },
      "source": [
        "## Using vanilla SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MdRWNshuHdNN",
        "colab_type": "code",
        "outputId": "e78d0199-3a10-4268-d1e4-d08f00b9f7c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# Unroll images into vectors\n",
        "X_train_r = X_train.reshape((X_train.shape[0], -1))\n",
        "X_val_r = X_val.reshape((X_val.shape[0], -1))\n",
        "print(X_train_r.shape)\n",
        "print(X_val_r.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(798, 1850)\n",
            "(171, 1850)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-8y-mN5EpKN",
        "colab_type": "code",
        "outputId": "1ec01c81-9a39-4a94-a3ac-680527de117e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "svc_clf = SVC()\n",
        "svc_clf.fit(X_train_r, y_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
              "    shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yRdBcjsnJTjt",
        "colab_type": "code",
        "outputId": "45d932be-bffd-458f-b198-b403ae919616",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_score = svc_clf.score(X_train_r, y_train)\n",
        "val_score = svc_clf.score(X_val_r, y_val)\n",
        "\n",
        "print(f\"Train score: {train_score * 100}\")\n",
        "print(f\"Val score: {val_score * 100}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train score: 100.0\n",
            "Val score: 47.953216374269005\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCIJ4WiJJtHO",
        "colab_type": "text"
      },
      "source": [
        "Obviously the model is overfitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7Eh9Og3FRMn",
        "colab_type": "text"
      },
      "source": [
        "### Grid search\n",
        "\n",
        "Let's try to get the best possible parameters to fit our dataset with SVM algorithm\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIG2QS8lFTAT",
        "colab_type": "code",
        "outputId": "f438d80a-59f9-43bf-bf23-40f5ff1765b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "params = {\n",
        "    'kernel': ('poly', 'rbf'),\n",
        "    'C': (1, 0.25, 0.5, 0.75),\n",
        "    'gamma': (1, 2, 3, 'auto'),\n",
        "    'decision_function_shape': ('ovo', 'ovr'),\n",
        "    'shrinking': (True, False)\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(estimator=svc_clf, param_grid=params, n_jobs=-1)\n",
        "clf.fit(X_train_r, y_train)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
              "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "                           decision_function_shape='ovr', degree=3,\n",
              "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
              "                           probability=False, random_state=None, shrinking=True,\n",
              "                           tol=0.001, verbose=False),\n",
              "             iid='warn', n_jobs=-1,\n",
              "             param_grid={'C': (1, 0.25, 0.5, 0.75),\n",
              "                         'decision_function_shape': ('ovo', 'ovr'),\n",
              "                         'gamma': (1, 2, 3, 'auto'), 'kernel': ('poly', 'rbf'),\n",
              "                         'shrinking': (True, False)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JGwdGX0ry72C",
        "colab_type": "code",
        "outputId": "12fb02fb-1e35-47ae-fac2-90c2acf04afc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(clf.cv_results_)\n",
        "\n",
        "print(clf.best_params_)\n",
        "print(clf.best_score_)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': 1, 'decision_function_shape': 'ovo', 'gamma': 1, 'kernel': 'poly', 'shrinking': True}\n",
            "0.8458646616541353\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtVYLMsXzeJe",
        "colab_type": "code",
        "outputId": "1f284b8f-f41c-49ca-bc8f-9d67a4e02b5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "grid_val_score = clf.score(X_val_r, y_val)\n",
        "print(grid_val_score)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8245614035087719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_cEuvq-xFq2",
        "colab_type": "text"
      },
      "source": [
        "We have resolved overfitting since now train and error scores are closer, however we still suffer a little of high bias, let's try some strategy to reduce it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2386Eot3KJ1P",
        "colab_type": "text"
      },
      "source": [
        "## Using PCA before SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wsTwRAj6E1Hs",
        "colab_type": "code",
        "outputId": "153edc82-fb50-44b9-c46a-52108e622dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 388
        }
      },
      "source": [
        "n_components = 100\n",
        "\n",
        "print(\"Extracting the top %d eigenfaces from %d faces\"\n",
        "      % (n_components, X_train_r.shape[0]))\n",
        "\n",
        "pca = PCA(n_components=n_components, svd_solver='randomized', whiten=True)\n",
        "pca = pca.fit(X_train_r)\n",
        "\n",
        "eigenfaces = pca.components_.reshape((n_components, height, width))\n",
        "plt.matshow(eigenfaces[0])\n",
        "plt.show()\n",
        "print(\"Projecting the input data on the eigenfaces orthonormal basis\")\n",
        "\n",
        "X_train_pca = pca.transform(X_train_r)\n",
        "X_val_pca = pca.transform(X_val_r)\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Extracting the top 100 eigenfaces from 798 faces\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQQAAAFQCAYAAABZFwDrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 288x389.189 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Projecting the input data on the eigenfaces orthonormal basis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3QKk9_0NTn-",
        "colab_type": "code",
        "outputId": "d32fa430-2b69-4bc0-af93-5ff811889d0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "source": [
        "svc_pca_clf = SVC()\n",
        "svc_pca_clf.fit(X_train_pca, y_train)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/svm/base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
            "  \"avoid this warning.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
              "    kernel='rbf', max_iter=-1, probability=False, random_state=None,\n",
              "    shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6FukqknKOYR",
        "colab_type": "code",
        "outputId": "e2b1d322-f4b6-4b57-cc5b-b67eac990b31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "train_score_pca = svc_pca_clf.score(X_train_pca, y_train)\n",
        "val_score_pca = svc_pca_clf.score(X_val_pca, y_val)\n",
        "print(f\"Train score: {train_score_pca}\")\n",
        "print(f\"Val score: {val_score_pca}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train score: 0.9912280701754386\n",
            "Val score: 0.8362573099415205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wNxrukEz2QJ9",
        "colab_type": "code",
        "outputId": "38a8a4b2-9250-4247-ba4f-05bdb89f06b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "params = {\n",
        "    'kernel': ('poly', 'rbf'),\n",
        "    'C': (1, 0.25, 0.5, 0.75),\n",
        "    'gamma': (1, 2, 3, 'auto'),\n",
        "    'decision_function_shape': ('ovo', 'ovr'),\n",
        "    'shrinking': (True, False)\n",
        "}\n",
        "\n",
        "clf = GridSearchCV(estimator=svc_pca_clf, param_grid=params, n_jobs=-1)\n",
        "clf.fit(X_train_pca, y_train)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:1978: FutureWarning: The default value of cv will change from 3 to 5 in version 0.22. Specify it explicitly to silence this warning.\n",
            "  warnings.warn(CV_WARNING, FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
              "             estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
              "                           decision_function_shape='ovr', degree=3,\n",
              "                           gamma='auto_deprecated', kernel='rbf', max_iter=-1,\n",
              "                           probability=False, random_state=None, shrinking=True,\n",
              "                           tol=0.001, verbose=False),\n",
              "             iid='warn', n_jobs=-1,\n",
              "             param_grid={'C': (1, 0.25, 0.5, 0.75),\n",
              "                         'decision_function_shape': ('ovo', 'ovr'),\n",
              "                         'gamma': (1, 2, 3, 'auto'), 'kernel': ('poly', 'rbf'),\n",
              "                         'shrinking': (True, False)},\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring=None, verbose=0)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_PDpoZE2fd0",
        "colab_type": "code",
        "outputId": "2f642cb1-828c-4cb2-a72d-8c2fdcf6489e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(clf.cv_results_)\n",
        "print(clf.best_params_)\n",
        "print(clf.best_score_)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'C': 1, 'decision_function_shape': 'ovo', 'gamma': 'auto', 'kernel': 'rbf', 'shrinking': True}\n",
            "0.7744360902255639\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMNXulSn2hoG",
        "colab_type": "code",
        "outputId": "d0c1f7d1-11bd-42a6-eff0-d61b7b71604a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "grid_val_score = clf.score(X_val_pca, y_val)\n",
        "print(grid_val_score)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.8362573099415205\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNDzYQ95vvXM",
        "colab_type": "text"
      },
      "source": [
        "Only by using PCA we got great results without too much tuning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QJUQQuK64Pm",
        "colab_type": "text"
      },
      "source": [
        "## Using neural networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw80aGjy61Jw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "layers = tf.keras.layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ya85QQ_P-7I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(input_units, num_labels):\n",
        "  model = tf.keras.models.Sequential([\n",
        "        layers.Dense(300, activation='relu', input_shape=(input_units, )),\n",
        "        layers.Dropout(rate=0.4),  \n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dense(128, activation='relu'),\n",
        "        layers.Dropout(rate=0.4),\n",
        "        layers.Dense(300, activation='relu'),\n",
        "        layers.Dropout(rate=0.4),\n",
        "        layers.Dense(num_labels, activation='softmax'),\n",
        "    ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_vELPjJARf1M",
        "colab_type": "code",
        "outputId": "a955c6e9-af11-4469-bb9e-01f1fc8523a9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "from sklearn.preprocessing import label_binarize\n",
        "import numpy as np\n",
        "\n",
        "# Encode labels\n",
        "classes = list(range(k_classes))\n",
        "y_train_tr = label_binarize(y_train, classes=classes)\n",
        "y_val_tr = label_binarize(y_val, classes=classes)\n",
        "\"\"\"\n",
        "Normalize features\n",
        "This is an usual trick for image data normalization\n",
        "the goal is to avoid computing mean and std from each channel\n",
        "\"\"\"\n",
        "X_train_scaled = X_train_r / 255.0\n",
        "X_val_scaled = X_val_r / 255.0\n",
        "\n",
        "print(np.mean(X_train_scaled), np.std(X_train_scaled))\n",
        "print(X_train_scaled.shape)\n",
        "print(X_val_scaled.shape)\n",
        "print(y_train_tr.shape)\n",
        "print(y_val_tr.shape)\n",
        "\n",
        "print('Encoding example')\n",
        "print(y_train[3])\n",
        "print(y_train_tr[3])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.52047396 0.1737373\n",
            "(798, 1850)\n",
            "(171, 1850)\n",
            "(798, 5)\n",
            "(171, 5)\n",
            "Encoding example\n",
            "0\n",
            "[1 0 0 0 0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wUN44UT3cbVb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_ds = tf.data.Dataset.from_tensor_slices(\n",
        "    (X_train_scaled, y_train_tr)).shuffle(800).batch(32)\n",
        "val_ds = tf.data.Dataset.from_tensor_slices((X_val_scaled, y_val_tr)).batch(32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YcrDr2kqblfd",
        "colab_type": "code",
        "outputId": "0066e128-38fe-448d-ed7b-78fc9c3f4fa6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model = build_model(input_units=features, num_labels=k_classes)\n",
        "#print(model.summary())\n",
        "loss_object = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.CategoricalAccuracy(name='train_accuracy')\n",
        "\n",
        "test_loss = tf.keras.metrics.Mean(name='test_loss')\n",
        "test_accuracy = tf.keras.metrics.CategoricalAccuracy(name='test_accuracy')\n",
        "EPOCHS = 500\n",
        "\n",
        "@tf.function\n",
        "def train_step(images, labels):\n",
        "  with tf.GradientTape() as tape:\n",
        "      logits = model(images)\n",
        "      loss_value = loss_object(labels, logits)\n",
        "  grads = tape.gradient(loss_value, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
        "  \n",
        "  train_loss(loss_value)\n",
        "  train_accuracy(labels, logits)\n",
        "\n",
        "@tf.function\n",
        "def test_step(images, labels):\n",
        "  logits = model(images)\n",
        "  t_loss = loss_object(labels, logits)\n",
        "\n",
        "  test_loss(t_loss)\n",
        "  test_accuracy(labels, logits)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  for images, labels in train_ds:\n",
        "    train_step(images, labels)\n",
        "  for images, labels in val_ds:\n",
        "    test_step(images, labels)\n",
        "  \n",
        "  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'\n",
        "  print (template.format(epoch+1,\n",
        "                         train_loss.result(),\n",
        "                         train_accuracy.result()*100,\n",
        "                         test_loss.result(),\n",
        "                         test_accuracy.result()*100))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1, Loss: 1.4984318017959595, Accuracy: 42.60651779174805, Test Loss: 1.452796459197998, Test Accuracy: 47.953216552734375\n",
            "Epoch 2, Loss: 1.4645171165466309, Accuracy: 44.98746871948242, Test Loss: 1.4417880773544312, Test Accuracy: 49.41520309448242\n",
            "Epoch 3, Loss: 1.4176069498062134, Accuracy: 47.36842346191406, Test Loss: 1.3989704847335815, Test Accuracy: 51.07212829589844\n",
            "Epoch 4, Loss: 1.390507698059082, Accuracy: 48.46491241455078, Test Loss: 1.3632878065109253, Test Accuracy: 52.6315803527832\n",
            "Epoch 5, Loss: 1.3430708646774292, Accuracy: 50.30075454711914, Test Loss: 1.3270009756088257, Test Accuracy: 53.918128967285156\n",
            "Epoch 6, Loss: 1.3058563470840454, Accuracy: 51.79615783691406, Test Loss: 1.2917537689208984, Test Accuracy: 55.0682258605957\n",
            "Epoch 7, Loss: 1.2669700384140015, Accuracy: 53.168636322021484, Test Loss: 1.2614665031433105, Test Accuracy: 56.05680847167969\n",
            "Epoch 8, Loss: 1.231195330619812, Accuracy: 54.46428680419922, Test Loss: 1.2375296354293823, Test Accuracy: 56.5058479309082\n",
            "Epoch 9, Loss: 1.1978325843811035, Accuracy: 55.569480895996094, Test Loss: 1.2080618143081665, Test Accuracy: 57.30994415283203\n",
            "Epoch 10, Loss: 1.1616462469100952, Accuracy: 56.75438690185547, Test Loss: 1.1732690334320068, Test Accuracy: 58.596492767333984\n",
            "Epoch 11, Loss: 1.1250717639923096, Accuracy: 58.03144073486328, Test Loss: 1.1462481021881104, Test Accuracy: 59.48963165283203\n",
            "Epoch 12, Loss: 1.1017097234725952, Accuracy: 58.84503173828125, Test Loss: 1.1147208213806152, Test Accuracy: 60.623783111572266\n",
            "Epoch 13, Loss: 1.0707436800003052, Accuracy: 60.16001892089844, Test Loss: 1.0869500637054443, Test Accuracy: 61.358524322509766\n",
            "Epoch 14, Loss: 1.0410614013671875, Accuracy: 61.331905364990234, Test Loss: 1.0654832124710083, Test Accuracy: 62.03007507324219\n",
            "Epoch 15, Loss: 1.0133899450302124, Accuracy: 62.35588836669922, Test Loss: 1.03965425491333, Test Accuracy: 62.96295928955078\n",
            "Epoch 16, Loss: 0.9809265732765198, Accuracy: 63.659149169921875, Test Loss: 1.0159847736358643, Test Accuracy: 63.70614242553711\n",
            "Epoch 17, Loss: 0.9486395716667175, Accuracy: 64.91228485107422, Test Loss: 0.990880012512207, Test Accuracy: 64.67147827148438\n",
            "Epoch 18, Loss: 0.917929470539093, Accuracy: 66.09579467773438, Test Loss: 0.9680819511413574, Test Accuracy: 65.43209838867188\n",
            "Epoch 19, Loss: 0.8898711204528809, Accuracy: 67.24706268310547, Test Loss: 0.9460292458534241, Test Accuracy: 66.26654052734375\n",
            "Epoch 20, Loss: 0.8728821873664856, Accuracy: 67.87593841552734, Test Loss: 0.9388376474380493, Test Accuracy: 66.57894897460938\n",
            "Epoch 21, Loss: 0.8613476157188416, Accuracy: 68.35540771484375, Test Loss: 0.9270204305648804, Test Accuracy: 66.91729736328125\n",
            "Epoch 22, Loss: 0.8495280742645264, Accuracy: 68.81977844238281, Test Loss: 0.9221741557121277, Test Accuracy: 67.03881072998047\n",
            "Epoch 23, Loss: 0.8335157036781311, Accuracy: 69.43445587158203, Test Loss: 0.9158117175102234, Test Accuracy: 67.27688598632812\n",
            "Epoch 24, Loss: 0.8161225318908691, Accuracy: 70.05535125732422, Test Loss: 0.9083902835845947, Test Accuracy: 67.49512481689453\n",
            "Epoch 25, Loss: 0.8021136522293091, Accuracy: 70.5614013671875, Test Loss: 0.9082788825035095, Test Accuracy: 67.46198272705078\n",
            "Epoch 26, Loss: 0.7919501066207886, Accuracy: 70.93695831298828, Test Loss: 0.9127786159515381, Test Accuracy: 67.36392211914062\n",
            "Epoch 27, Loss: 0.777298092842102, Accuracy: 71.56781005859375, Test Loss: 0.9163779020309448, Test Accuracy: 67.48971557617188\n",
            "Epoch 28, Loss: 0.7602827548980713, Accuracy: 72.23416137695312, Test Loss: 0.9097995758056641, Test Accuracy: 67.8780288696289\n",
            "Epoch 29, Loss: 0.7433673143386841, Accuracy: 72.83726501464844, Test Loss: 0.9034187197685242, Test Accuracy: 68.25972747802734\n",
            "Epoch 30, Loss: 0.7286385893821716, Accuracy: 73.37510681152344, Test Loss: 0.9022745490074158, Test Accuracy: 68.42105102539062\n",
            "Epoch 31, Loss: 0.7150493264198303, Accuracy: 73.85398864746094, Test Loss: 0.9071699380874634, Test Accuracy: 68.42105102539062\n",
            "Epoch 32, Loss: 0.7027878761291504, Accuracy: 74.3538589477539, Test Loss: 0.8975237011909485, Test Accuracy: 68.76827239990234\n",
            "Epoch 33, Loss: 0.6907872557640076, Accuracy: 74.79684448242188, Test Loss: 0.8946972489356995, Test Accuracy: 68.88179779052734\n",
            "Epoch 34, Loss: 0.6774966716766357, Accuracy: 75.27642822265625, Test Loss: 0.8880575299263, Test Accuracy: 69.14344787597656\n",
            "Epoch 35, Loss: 0.6647362112998962, Accuracy: 75.75009155273438, Test Loss: 0.8807801604270935, Test Accuracy: 69.47368621826172\n",
            "Epoch 36, Loss: 0.652248203754425, Accuracy: 76.19047546386719, Test Loss: 0.8793259859085083, Test Accuracy: 69.6881103515625\n",
            "Epoch 37, Loss: 0.6403939127922058, Accuracy: 76.63414764404297, Test Loss: 0.8764598965644836, Test Accuracy: 69.95416259765625\n",
            "Epoch 38, Loss: 0.6304056644439697, Accuracy: 77.00831604003906, Test Loss: 0.8723294734954834, Test Accuracy: 70.25238037109375\n",
            "Epoch 39, Loss: 0.6214402914047241, Accuracy: 77.34078979492188, Test Loss: 0.8692017197608948, Test Accuracy: 70.43035125732422\n",
            "Epoch 40, Loss: 0.6172560453414917, Accuracy: 77.52506256103516, Test Loss: 0.860824465751648, Test Accuracy: 70.7602310180664\n",
            "Epoch 41, Loss: 0.6102728843688965, Accuracy: 77.81649017333984, Test Loss: 0.859015941619873, Test Accuracy: 70.80302429199219\n",
            "Epoch 42, Loss: 0.6035811305046082, Accuracy: 78.05824279785156, Test Loss: 0.8541257977485657, Test Accuracy: 70.96908569335938\n",
            "Epoch 43, Loss: 0.5977733135223389, Accuracy: 78.24211883544922, Test Loss: 0.8548468947410583, Test Accuracy: 70.9642333984375\n",
            "Epoch 44, Loss: 0.5920912027359009, Accuracy: 78.4603500366211, Test Loss: 0.8656470775604248, Test Accuracy: 70.77352142333984\n",
            "Epoch 45, Loss: 0.5909395217895508, Accuracy: 78.4544677734375, Test Loss: 0.8619775176048279, Test Accuracy: 70.85120391845703\n",
            "Epoch 46, Loss: 0.5866315960884094, Accuracy: 78.62863159179688, Test Loss: 0.8632507920265198, Test Accuracy: 70.88735961914062\n",
            "Epoch 47, Loss: 0.5805169343948364, Accuracy: 78.8407211303711, Test Loss: 0.8725191950798035, Test Accuracy: 70.74779510498047\n",
            "Epoch 48, Loss: 0.5775046348571777, Accuracy: 78.93431854248047, Test Loss: 0.8686890006065369, Test Accuracy: 70.82115173339844\n",
            "Epoch 49, Loss: 0.572748064994812, Accuracy: 79.12383270263672, Test Loss: 0.8655372858047485, Test Accuracy: 70.98699188232422\n",
            "Epoch 50, Loss: 0.5661554932594299, Accuracy: 79.3583984375, Test Loss: 0.8690827488899231, Test Accuracy: 71.00584411621094\n",
            "Epoch 51, Loss: 0.5640603303909302, Accuracy: 79.43634033203125, Test Loss: 0.8677324056625366, Test Accuracy: 71.03543090820312\n",
            "Epoch 52, Loss: 0.5590118169784546, Accuracy: 79.63658905029297, Test Loss: 0.8683996796607971, Test Accuracy: 71.12010955810547\n",
            "Epoch 53, Loss: 0.5530650019645691, Accuracy: 79.85057067871094, Test Loss: 0.8763447999954224, Test Accuracy: 71.08021545410156\n",
            "Epoch 54, Loss: 0.5495362877845764, Accuracy: 79.96843719482422, Test Loss: 0.8730054497718811, Test Accuracy: 71.23673248291016\n",
            "Epoch 55, Loss: 0.5437508821487427, Accuracy: 80.18455505371094, Test Loss: 0.8743211030960083, Test Accuracy: 71.3131332397461\n",
            "Epoch 56, Loss: 0.5373505353927612, Accuracy: 80.41085052490234, Test Loss: 0.8739282488822937, Test Accuracy: 71.43901824951172\n",
            "Epoch 57, Loss: 0.5315122008323669, Accuracy: 80.63360595703125, Test Loss: 0.8705326318740845, Test Accuracy: 71.60151672363281\n",
            "Epoch 58, Loss: 0.5253100395202637, Accuracy: 80.86811828613281, Test Loss: 0.8747268319129944, Test Accuracy: 71.607177734375\n",
            "Epoch 59, Loss: 0.5193264484405518, Accuracy: 81.07981872558594, Test Loss: 0.8805269598960876, Test Accuracy: 71.6126480102539\n",
            "Epoch 60, Loss: 0.5131610631942749, Accuracy: 81.30535125732422, Test Loss: 0.8800693154335022, Test Accuracy: 71.71539306640625\n",
            "Epoch 61, Loss: 0.5067859888076782, Accuracy: 81.54813385009766, Test Loss: 0.8781963586807251, Test Accuracy: 71.88188934326172\n",
            "Epoch 62, Loss: 0.5019022226333618, Accuracy: 81.73660278320312, Test Loss: 0.8776232600212097, Test Accuracy: 71.97698211669922\n",
            "Epoch 63, Loss: 0.49629151821136475, Accuracy: 81.9330062866211, Test Loss: 0.8786317706108093, Test Accuracy: 72.04121398925781\n",
            "Epoch 64, Loss: 0.49051129817962646, Accuracy: 82.14677429199219, Test Loss: 0.883754312992096, Test Accuracy: 72.08515930175781\n",
            "Epoch 65, Loss: 0.4855121374130249, Accuracy: 82.33082580566406, Test Loss: 0.8963925838470459, Test Accuracy: 72.02879333496094\n",
            "Epoch 66, Loss: 0.4809083044528961, Accuracy: 82.49221801757812, Test Loss: 0.8976748585700989, Test Accuracy: 72.124755859375\n",
            "Epoch 67, Loss: 0.4773443937301636, Accuracy: 82.6207275390625, Test Loss: 0.9001178741455078, Test Accuracy: 72.11312103271484\n",
            "Epoch 68, Loss: 0.4739845097064972, Accuracy: 82.75836944580078, Test Loss: 0.9066701531410217, Test Accuracy: 72.0416259765625\n",
            "Epoch 69, Loss: 0.4701075851917267, Accuracy: 82.90109252929688, Test Loss: 0.9049240350723267, Test Accuracy: 72.124755859375\n",
            "Epoch 70, Loss: 0.46559131145477295, Accuracy: 83.07017517089844, Test Loss: 0.9031226634979248, Test Accuracy: 72.2138671875\n",
            "Epoch 71, Loss: 0.46107280254364014, Accuracy: 83.22920227050781, Test Loss: 0.9023848176002502, Test Accuracy: 72.31694793701172\n",
            "Epoch 72, Loss: 0.4570043385028839, Accuracy: 83.36640167236328, Test Loss: 0.9094703197479248, Test Accuracy: 72.32781219482422\n",
            "Epoch 73, Loss: 0.45396673679351807, Accuracy: 83.47924041748047, Test Loss: 0.907635509967804, Test Accuracy: 72.44252014160156\n",
            "Epoch 74, Loss: 0.4497358798980713, Accuracy: 83.63983917236328, Test Loss: 0.9083897471427917, Test Accuracy: 72.51461791992188\n",
            "Epoch 75, Loss: 0.4453461766242981, Accuracy: 83.8011703491211, Test Loss: 0.907692015171051, Test Accuracy: 72.58479309082031\n",
            "Epoch 76, Loss: 0.4409591853618622, Accuracy: 83.95989990234375, Test Loss: 0.9129204154014587, Test Accuracy: 72.57617950439453\n",
            "Epoch 77, Loss: 0.4375590980052948, Accuracy: 84.07870483398438, Test Loss: 0.9126805067062378, Test Accuracy: 72.68930053710938\n",
            "Epoch 78, Loss: 0.43499118089675903, Accuracy: 84.17356872558594, Test Loss: 0.9152868986129761, Test Accuracy: 72.73204040527344\n",
            "Epoch 79, Loss: 0.43183305859565735, Accuracy: 84.28825378417969, Test Loss: 0.9201547503471375, Test Accuracy: 72.71448516845703\n",
            "Epoch 80, Loss: 0.42809441685676575, Accuracy: 84.42512512207031, Test Loss: 0.9173961281776428, Test Accuracy: 72.80702209472656\n",
            "Epoch 81, Loss: 0.4239291846752167, Accuracy: 84.5880126953125, Test Loss: 0.9167858362197876, Test Accuracy: 72.89004516601562\n",
            "Epoch 82, Loss: 0.41988250613212585, Accuracy: 84.7362289428711, Test Loss: 0.9218811988830566, Test Accuracy: 72.92825317382812\n",
            "Epoch 83, Loss: 0.416452020406723, Accuracy: 84.85823059082031, Test Loss: 0.9212932586669922, Test Accuracy: 73.04304504394531\n",
            "Epoch 84, Loss: 0.41266414523124695, Accuracy: 84.99970245361328, Test Loss: 0.9277615547180176, Test Accuracy: 72.97410583496094\n",
            "Epoch 85, Loss: 0.41059425473213196, Accuracy: 85.08771514892578, Test Loss: 0.9278268814086914, Test Accuracy: 73.03749084472656\n",
            "Epoch 86, Loss: 0.4068778455257416, Accuracy: 85.22468566894531, Test Loss: 0.9261336922645569, Test Accuracy: 73.14701843261719\n",
            "Epoch 87, Loss: 0.403079628944397, Accuracy: 85.36138916015625, Test Loss: 0.9249683618545532, Test Accuracy: 73.26073455810547\n",
            "Epoch 88, Loss: 0.39937329292297363, Accuracy: 85.49498748779297, Test Loss: 0.9248405694961548, Test Accuracy: 73.37187194824219\n",
            "Epoch 89, Loss: 0.3958556652069092, Accuracy: 85.63262176513672, Test Loss: 0.925735592842102, Test Accuracy: 73.42794799804688\n",
            "Epoch 90, Loss: 0.3929891884326935, Accuracy: 85.74213409423828, Test Loss: 0.9272890686988831, Test Accuracy: 73.49577331542969\n",
            "Epoch 91, Loss: 0.39011499285697937, Accuracy: 85.85061645507812, Test Loss: 0.9267851710319519, Test Accuracy: 73.54925537109375\n",
            "Epoch 92, Loss: 0.38656115531921387, Accuracy: 85.98534393310547, Test Loss: 0.9268577098846436, Test Accuracy: 73.63336181640625\n",
            "Epoch 93, Loss: 0.3830723762512207, Accuracy: 86.1158218383789, Test Loss: 0.926155686378479, Test Accuracy: 73.74080657958984\n",
            "Epoch 94, Loss: 0.3801634907722473, Accuracy: 86.21687316894531, Test Loss: 0.9305288791656494, Test Accuracy: 73.77130889892578\n",
            "Epoch 95, Loss: 0.37705421447753906, Accuracy: 86.3302993774414, Test Loss: 0.9324318170547485, Test Accuracy: 73.80731964111328\n",
            "Epoch 96, Loss: 0.3741510510444641, Accuracy: 86.4348373413086, Test Loss: 0.9321807026863098, Test Accuracy: 73.8852310180664\n",
            "Epoch 97, Loss: 0.3714381754398346, Accuracy: 86.53205108642578, Test Loss: 0.933240532875061, Test Accuracy: 73.93742370605469\n",
            "Epoch 98, Loss: 0.3695318102836609, Accuracy: 86.60554504394531, Test Loss: 0.9336210489273071, Test Accuracy: 73.98257446289062\n",
            "Epoch 99, Loss: 0.3668712079524994, Accuracy: 86.70159912109375, Test Loss: 0.9341058731079102, Test Accuracy: 74.00909423828125\n",
            "Epoch 100, Loss: 0.3652735650539398, Accuracy: 86.77192687988281, Test Loss: 0.9426617622375488, Test Accuracy: 73.9356689453125\n",
            "Epoch 101, Loss: 0.3647482693195343, Accuracy: 86.79991912841797, Test Loss: 0.9425097107887268, Test Accuracy: 73.97949981689453\n",
            "Epoch 102, Loss: 0.36218997836112976, Accuracy: 86.89247131347656, Test Loss: 0.9405444860458374, Test Accuracy: 74.05687713623047\n",
            "Epoch 103, Loss: 0.35982462763786316, Accuracy: 86.9783706665039, Test Loss: 0.9396550059318542, Test Accuracy: 74.11571502685547\n",
            "Epoch 104, Loss: 0.35760506987571716, Accuracy: 87.0505599975586, Test Loss: 0.9382977485656738, Test Accuracy: 74.17341613769531\n",
            "Epoch 105, Loss: 0.35523077845573425, Accuracy: 87.13569641113281, Test Loss: 0.9378038048744202, Test Accuracy: 74.22444915771484\n",
            "Epoch 106, Loss: 0.3525121510028839, Accuracy: 87.23814392089844, Test Loss: 0.9395407438278198, Test Accuracy: 74.27452087402344\n",
            "Epoch 107, Loss: 0.34981706738471985, Accuracy: 87.33867645263672, Test Loss: 0.9400932192802429, Test Accuracy: 74.34005737304688\n",
            "Epoch 108, Loss: 0.34728100895881653, Accuracy: 87.43038177490234, Test Loss: 0.9411624073982239, Test Accuracy: 74.3827133178711\n",
            "Epoch 109, Loss: 0.3463628888130188, Accuracy: 87.46636962890625, Test Loss: 0.9464350342750549, Test Accuracy: 74.3280258178711\n",
            "Epoch 110, Loss: 0.3451888859272003, Accuracy: 87.51082611083984, Test Loss: 0.9452735781669617, Test Accuracy: 74.37532806396484\n",
            "Epoch 111, Loss: 0.3429831266403198, Accuracy: 87.5905990600586, Test Loss: 0.946098268032074, Test Accuracy: 74.4112548828125\n",
            "Epoch 112, Loss: 0.3406887650489807, Accuracy: 87.67118835449219, Test Loss: 0.9448434114456177, Test Accuracy: 74.47785949707031\n",
            "Epoch 113, Loss: 0.33910074830055237, Accuracy: 87.71929931640625, Test Loss: 0.9440155625343323, Test Accuracy: 74.54328918457031\n",
            "Epoch 114, Loss: 0.33719339966773987, Accuracy: 87.7841567993164, Test Loss: 0.9425442218780518, Test Accuracy: 74.61270141601562\n",
            "Epoch 115, Loss: 0.3357195258140564, Accuracy: 87.84025573730469, Test Loss: 0.9409183859825134, Test Accuracy: 74.67581939697266\n",
            "Epoch 116, Loss: 0.3344109058380127, Accuracy: 87.88134002685547, Test Loss: 0.9391646981239319, Test Accuracy: 74.7328109741211\n",
            "Epoch 117, Loss: 0.3336281478404999, Accuracy: 87.90030670166016, Test Loss: 0.939061164855957, Test Accuracy: 74.75383758544922\n",
            "Epoch 118, Loss: 0.3319828510284424, Accuracy: 87.95399475097656, Test Loss: 0.9385359883308411, Test Accuracy: 74.78441619873047\n",
            "Epoch 119, Loss: 0.3300236463546753, Accuracy: 88.02995300292969, Test Loss: 0.9379470944404602, Test Accuracy: 74.8439712524414\n",
            "Epoch 120, Loss: 0.3283558487892151, Accuracy: 88.09210968017578, Test Loss: 0.9377503395080566, Test Accuracy: 74.8635482788086\n",
            "Epoch 121, Loss: 0.32716450095176697, Accuracy: 88.1376953125, Test Loss: 0.9362348318099976, Test Accuracy: 74.9407958984375\n",
            "Epoch 122, Loss: 0.3253779709339142, Accuracy: 88.19384765625, Test Loss: 0.9351201057434082, Test Accuracy: 74.99760437011719\n",
            "Epoch 123, Loss: 0.32345521450042725, Accuracy: 88.25926971435547, Test Loss: 0.9358668327331543, Test Accuracy: 75.0534896850586\n",
            "Epoch 124, Loss: 0.3218879699707031, Accuracy: 88.31655883789062, Test Loss: 0.9354247450828552, Test Accuracy: 75.0801773071289\n",
            "Epoch 125, Loss: 0.32028183341026306, Accuracy: 88.37092590332031, Test Loss: 0.9370774626731873, Test Accuracy: 75.12046813964844\n",
            "Epoch 126, Loss: 0.318491667509079, Accuracy: 88.43437957763672, Test Loss: 0.9365273118019104, Test Accuracy: 75.16476440429688\n",
            "Epoch 127, Loss: 0.3165264427661896, Accuracy: 88.50373840332031, Test Loss: 0.9375616908073425, Test Accuracy: 75.2037582397461\n",
            "Epoch 128, Loss: 0.31452739238739014, Accuracy: 88.575927734375, Test Loss: 0.9397031664848328, Test Accuracy: 75.2330093383789\n",
            "Epoch 129, Loss: 0.3127989172935486, Accuracy: 88.63826751708984, Test Loss: 0.9426321387290955, Test Accuracy: 75.26179504394531\n",
            "Epoch 130, Loss: 0.311753511428833, Accuracy: 88.67938995361328, Test Loss: 0.9452276825904846, Test Accuracy: 75.24516296386719\n",
            "Epoch 131, Loss: 0.310160756111145, Accuracy: 88.74093627929688, Test Loss: 0.9465362429618835, Test Accuracy: 75.28235626220703\n",
            "Epoch 132, Loss: 0.3087192475795746, Accuracy: 88.79396057128906, Test Loss: 0.9492664337158203, Test Accuracy: 75.28353881835938\n",
            "Epoch 133, Loss: 0.30710265040397644, Accuracy: 88.85088348388672, Test Loss: 0.9511261582374573, Test Accuracy: 75.28469848632812\n",
            "Epoch 134, Loss: 0.3059106469154358, Accuracy: 88.88919830322266, Test Loss: 0.9533522129058838, Test Accuracy: 75.2945785522461\n",
            "Epoch 135, Loss: 0.304730623960495, Accuracy: 88.93158721923828, Test Loss: 0.9560505151748657, Test Accuracy: 75.29564666748047\n",
            "Epoch 136, Loss: 0.30355188250541687, Accuracy: 88.97058868408203, Test Loss: 0.9584066271781921, Test Accuracy: 75.28810119628906\n",
            "Epoch 137, Loss: 0.3039156496524811, Accuracy: 88.95870971679688, Test Loss: 0.967154324054718, Test Accuracy: 75.18247985839844\n",
            "Epoch 138, Loss: 0.30360856652259827, Accuracy: 88.96971130371094, Test Loss: 0.9663119316101074, Test Accuracy: 75.2012939453125\n",
            "Epoch 139, Loss: 0.30192476511001587, Accuracy: 89.0328369140625, Test Loss: 0.9677830934524536, Test Accuracy: 75.19036865234375\n",
            "Epoch 140, Loss: 0.3013007938861847, Accuracy: 89.05567169189453, Test Loss: 0.9692252278327942, Test Accuracy: 75.18379211425781\n",
            "Epoch 141, Loss: 0.3004271984100342, Accuracy: 89.0853042602539, Test Loss: 0.968536376953125, Test Accuracy: 75.22293090820312\n",
            "Epoch 142, Loss: 0.29904675483703613, Accuracy: 89.13834381103516, Test Loss: 0.9681500792503357, Test Accuracy: 75.26974487304688\n",
            "Epoch 143, Loss: 0.29777953028678894, Accuracy: 89.17924499511719, Test Loss: 0.9678595066070557, Test Accuracy: 75.30773162841797\n",
            "Epoch 144, Loss: 0.29633769392967224, Accuracy: 89.23001861572266, Test Loss: 0.9682449698448181, Test Accuracy: 75.32489013671875\n",
            "Epoch 145, Loss: 0.2950223982334137, Accuracy: 89.27750396728516, Test Loss: 0.9672951698303223, Test Accuracy: 75.37003326416016\n",
            "Epoch 146, Loss: 0.29375261068344116, Accuracy: 89.3243408203125, Test Loss: 0.9685076475143433, Test Accuracy: 75.39053344726562\n",
            "Epoch 147, Loss: 0.29274702072143555, Accuracy: 89.35859680175781, Test Loss: 0.9713871479034424, Test Accuracy: 75.37494659423828\n",
            "Epoch 148, Loss: 0.29162728786468506, Accuracy: 89.39747619628906, Test Loss: 0.973372220993042, Test Accuracy: 75.37142944335938\n",
            "Epoch 149, Loss: 0.29132765531539917, Accuracy: 89.4080810546875, Test Loss: 0.9724540710449219, Test Accuracy: 75.41112518310547\n",
            "Epoch 150, Loss: 0.29040682315826416, Accuracy: 89.4394302368164, Test Loss: 0.972531795501709, Test Accuracy: 75.43470001220703\n",
            "Epoch 151, Loss: 0.2893863320350647, Accuracy: 89.475341796875, Test Loss: 0.9729452133178711, Test Accuracy: 75.44246673583984\n",
            "Epoch 152, Loss: 0.2890468239784241, Accuracy: 89.48686981201172, Test Loss: 0.9732110500335693, Test Accuracy: 75.46937561035156\n",
            "Epoch 153, Loss: 0.28796374797821045, Accuracy: 89.52446746826172, Test Loss: 0.9716161489486694, Test Accuracy: 75.51121520996094\n",
            "Epoch 154, Loss: 0.28640487790107727, Accuracy: 89.58272552490234, Test Loss: 0.9722896814346313, Test Accuracy: 75.53353118896484\n",
            "Epoch 155, Loss: 0.28476017713546753, Accuracy: 89.64103698730469, Test Loss: 0.9748063683509827, Test Accuracy: 75.5517807006836\n",
            "Epoch 156, Loss: 0.2832138240337372, Accuracy: 89.69940948486328, Test Loss: 0.9757035374641418, Test Accuracy: 75.5698013305664\n",
            "Epoch 157, Loss: 0.28161242604255676, Accuracy: 89.7578353881836, Test Loss: 0.9764019250869751, Test Accuracy: 75.60621643066406\n",
            "Epoch 158, Loss: 0.27993762493133545, Accuracy: 89.82027435302734, Test Loss: 0.9777758121490479, Test Accuracy: 75.63845825195312\n",
            "Epoch 159, Loss: 0.2783713638782501, Accuracy: 89.8787841796875, Test Loss: 0.9784532189369202, Test Accuracy: 75.66663360595703\n",
            "Epoch 160, Loss: 0.27683499455451965, Accuracy: 89.93498992919922, Test Loss: 0.9787581562995911, Test Accuracy: 75.70906066894531\n",
            "Epoch 161, Loss: 0.2753385007381439, Accuracy: 89.99128723144531, Test Loss: 0.9849673509597778, Test Accuracy: 75.70738220214844\n",
            "Epoch 162, Loss: 0.2738989591598511, Accuracy: 90.0445556640625, Test Loss: 0.98597252368927, Test Accuracy: 75.7418212890625\n",
            "Epoch 163, Loss: 0.2724587619304657, Accuracy: 90.09640502929688, Test Loss: 0.9885043501853943, Test Accuracy: 75.75431823730469\n",
            "Epoch 164, Loss: 0.27143821120262146, Accuracy: 90.13311004638672, Test Loss: 0.9877960085868835, Test Accuracy: 75.78447723388672\n",
            "Epoch 165, Loss: 0.2701391577720642, Accuracy: 90.1822738647461, Test Loss: 0.9909822940826416, Test Accuracy: 75.78947448730469\n",
            "Epoch 166, Loss: 0.2687276601791382, Accuracy: 90.23160552978516, Test Loss: 0.9937295317649841, Test Accuracy: 75.79087829589844\n",
            "Epoch 167, Loss: 0.26736241579055786, Accuracy: 90.28108978271484, Test Loss: 0.9946584701538086, Test Accuracy: 75.82379150390625\n",
            "Epoch 168, Loss: 0.2664089500904083, Accuracy: 90.3158187866211, Test Loss: 0.9957813024520874, Test Accuracy: 75.84934997558594\n",
            "Epoch 169, Loss: 0.26664358377456665, Accuracy: 90.31973266601562, Test Loss: 0.9980019927024841, Test Accuracy: 75.80538940429688\n",
            "Epoch 170, Loss: 0.2659800946712494, Accuracy: 90.343505859375, Test Loss: 0.9997091293334961, Test Accuracy: 75.7791519165039\n",
            "Epoch 171, Loss: 0.2648228704929352, Accuracy: 90.38678741455078, Test Loss: 0.998359203338623, Test Accuracy: 75.81819915771484\n",
            "Epoch 172, Loss: 0.2634485065937042, Accuracy: 90.43903350830078, Test Loss: 0.9980597496032715, Test Accuracy: 75.83979034423828\n",
            "Epoch 173, Loss: 0.2621811032295227, Accuracy: 90.48416137695312, Test Loss: 0.9982670545578003, Test Accuracy: 75.86113739013672\n",
            "Epoch 174, Loss: 0.26085183024406433, Accuracy: 90.5345230102539, Test Loss: 0.9993294477462769, Test Accuracy: 75.86206817626953\n",
            "Epoch 175, Loss: 0.2595345079898834, Accuracy: 90.58360290527344, Test Loss: 0.9992050528526306, Test Accuracy: 75.89974975585938\n",
            "Epoch 176, Loss: 0.2581256330013275, Accuracy: 90.63568115234375, Test Loss: 1.0007065534591675, Test Accuracy: 75.93367767333984\n",
            "Epoch 177, Loss: 0.2568463385105133, Accuracy: 90.68009185791016, Test Loss: 1.0014132261276245, Test Accuracy: 75.96392059326172\n",
            "Epoch 178, Loss: 0.2555302381515503, Accuracy: 90.72822570800781, Test Loss: 1.002646803855896, Test Accuracy: 75.98068237304688\n",
            "Epoch 179, Loss: 0.25415635108947754, Accuracy: 90.77862548828125, Test Loss: 1.0031020641326904, Test Accuracy: 76.02012634277344\n",
            "Epoch 180, Loss: 0.2528504729270935, Accuracy: 90.82706451416016, Test Loss: 1.0055104494094849, Test Accuracy: 76.04288482666016\n",
            "Epoch 181, Loss: 0.2515340745449066, Accuracy: 90.87359619140625, Test Loss: 1.0080288648605347, Test Accuracy: 76.06861877441406\n",
            "Epoch 182, Loss: 0.25025564432144165, Accuracy: 90.91960906982422, Test Loss: 1.0102344751358032, Test Accuracy: 76.0972900390625\n",
            "Epoch 183, Loss: 0.2499125599861145, Accuracy: 90.93362426757812, Test Loss: 1.0122616291046143, Test Accuracy: 76.11286926269531\n",
            "Epoch 184, Loss: 0.25010523200035095, Accuracy: 90.9324951171875, Test Loss: 1.0120149850845337, Test Accuracy: 76.13462829589844\n",
            "Epoch 185, Loss: 0.2490748018026352, Accuracy: 90.9720230102539, Test Loss: 1.0154763460159302, Test Accuracy: 76.11190032958984\n",
            "Epoch 186, Loss: 0.24799862504005432, Accuracy: 91.01248168945312, Test Loss: 1.017982840538025, Test Accuracy: 76.10513305664062\n",
            "Epoch 187, Loss: 0.24681797623634338, Accuracy: 91.05517578125, Test Loss: 1.019162893295288, Test Accuracy: 76.13284301757812\n",
            "Epoch 188, Loss: 0.2460797280073166, Accuracy: 91.08275604248047, Test Loss: 1.0203098058700562, Test Accuracy: 76.1353759765625\n",
            "Epoch 189, Loss: 0.24576334655284882, Accuracy: 91.09811401367188, Test Loss: 1.0204248428344727, Test Accuracy: 76.15644073486328\n",
            "Epoch 190, Loss: 0.24488531053066254, Accuracy: 91.13046264648438, Test Loss: 1.0205942392349243, Test Accuracy: 76.17112731933594\n",
            "Epoch 191, Loss: 0.24370431900024414, Accuracy: 91.17427062988281, Test Loss: 1.0209442377090454, Test Accuracy: 76.19790649414062\n",
            "Epoch 192, Loss: 0.24256274104118347, Accuracy: 91.21566772460938, Test Loss: 1.0226428508758545, Test Accuracy: 76.22137451171875\n",
            "Epoch 193, Loss: 0.24139587581157684, Accuracy: 91.25923156738281, Test Loss: 1.0242512226104736, Test Accuracy: 76.25367736816406\n",
            "Epoch 194, Loss: 0.24023006856441498, Accuracy: 91.30105590820312, Test Loss: 1.0259007215499878, Test Accuracy: 76.27961730957031\n",
            "Epoch 195, Loss: 0.2390739917755127, Accuracy: 91.34374237060547, Test Loss: 1.0286062955856323, Test Accuracy: 76.29629516601562\n",
            "Epoch 196, Loss: 0.23793305456638336, Accuracy: 91.38599395751953, Test Loss: 1.0333772897720337, Test Accuracy: 76.29191589355469\n",
            "Epoch 197, Loss: 0.23683513700962067, Accuracy: 91.42526245117188, Test Loss: 1.0348840951919556, Test Accuracy: 76.32024383544922\n",
            "Epoch 198, Loss: 0.23607054352760315, Accuracy: 91.45211791992188, Test Loss: 1.0348128080368042, Test Accuracy: 76.33941650390625\n",
            "Epoch 199, Loss: 0.23621220886707306, Accuracy: 91.45413970947266, Test Loss: 1.038404107093811, Test Accuracy: 76.30844116210938\n",
            "Epoch 200, Loss: 0.23539084196090698, Accuracy: 91.48307800292969, Test Loss: 1.0394387245178223, Test Accuracy: 76.32748413085938\n",
            "Epoch 201, Loss: 0.23442649841308594, Accuracy: 91.51860046386719, Test Loss: 1.0405995845794678, Test Accuracy: 76.34052276611328\n",
            "Epoch 202, Loss: 0.2333637923002243, Accuracy: 91.55810546875, Test Loss: 1.04425847530365, Test Accuracy: 76.34184265136719\n",
            "Epoch 203, Loss: 0.23230023682117462, Accuracy: 91.59660339355469, Test Loss: 1.048094391822815, Test Accuracy: 76.34891510009766\n",
            "Epoch 204, Loss: 0.23126323521137238, Accuracy: 91.63533782958984, Test Loss: 1.048736810684204, Test Accuracy: 76.36739349365234\n",
            "Epoch 205, Loss: 0.23021143674850464, Accuracy: 91.67369842529297, Test Loss: 1.0515773296356201, Test Accuracy: 76.37712097167969\n",
            "Epoch 206, Loss: 0.22916461527347565, Accuracy: 91.71168518066406, Test Loss: 1.05397367477417, Test Accuracy: 76.38391876220703\n",
            "Epoch 207, Loss: 0.2281036227941513, Accuracy: 91.75111389160156, Test Loss: 1.0570110082626343, Test Accuracy: 76.39913177490234\n",
            "Epoch 208, Loss: 0.22707198560237885, Accuracy: 91.78836059570312, Test Loss: 1.0587238073349, Test Accuracy: 76.43668365478516\n",
            "Epoch 209, Loss: 0.22618110477924347, Accuracy: 91.82225799560547, Test Loss: 1.0606186389923096, Test Accuracy: 76.45709228515625\n",
            "Epoch 210, Loss: 0.22518490254878998, Accuracy: 91.85762023925781, Test Loss: 1.0624440908432007, Test Accuracy: 76.4661636352539\n",
            "Epoch 211, Loss: 0.22446990013122559, Accuracy: 91.88314819335938, Test Loss: 1.0630834102630615, Test Accuracy: 76.49178314208984\n",
            "Epoch 212, Loss: 0.22363698482513428, Accuracy: 91.91197204589844, Test Loss: 1.0628739595413208, Test Accuracy: 76.51164245605469\n",
            "Epoch 213, Loss: 0.22266319394111633, Accuracy: 91.94876861572266, Test Loss: 1.0647081136703491, Test Accuracy: 76.52307891845703\n",
            "Epoch 214, Loss: 0.22168458998203278, Accuracy: 91.98345947265625, Test Loss: 1.0687028169631958, Test Accuracy: 76.52346801757812\n",
            "Epoch 215, Loss: 0.22082260251045227, Accuracy: 92.01433563232422, Test Loss: 1.0720912218093872, Test Accuracy: 76.5265884399414\n",
            "Epoch 216, Loss: 0.22029311954975128, Accuracy: 92.03622436523438, Test Loss: 1.0791844129562378, Test Accuracy: 76.46469116210938\n",
            "Epoch 217, Loss: 0.22013641893863678, Accuracy: 92.04520416259766, Test Loss: 1.078184962272644, Test Accuracy: 76.47613525390625\n",
            "Epoch 218, Loss: 0.21943962574005127, Accuracy: 92.07019805908203, Test Loss: 1.0784367322921753, Test Accuracy: 76.49820709228516\n",
            "Epoch 219, Loss: 0.21857890486717224, Accuracy: 92.1018295288086, Test Loss: 1.0781643390655518, Test Accuracy: 76.52006530761719\n",
            "Epoch 220, Loss: 0.21765580773353577, Accuracy: 92.13545227050781, Test Loss: 1.0801066160202026, Test Accuracy: 76.52046966552734\n",
            "Epoch 221, Loss: 0.21674756705760956, Accuracy: 92.16934204101562, Test Loss: 1.0819553136825562, Test Accuracy: 76.53144836425781\n",
            "Epoch 222, Loss: 0.21591337025165558, Accuracy: 92.19953155517578, Test Loss: 1.081832766532898, Test Accuracy: 76.55813598632812\n",
            "Epoch 223, Loss: 0.2150714248418808, Accuracy: 92.23057556152344, Test Loss: 1.0823291540145874, Test Accuracy: 76.5740966796875\n",
            "Epoch 224, Loss: 0.2143624871969223, Accuracy: 92.258544921875, Test Loss: 1.0835466384887695, Test Accuracy: 76.5794677734375\n",
            "Epoch 225, Loss: 0.2135714590549469, Accuracy: 92.28738403320312, Test Loss: 1.0842386484146118, Test Accuracy: 76.61078643798828\n",
            "Epoch 226, Loss: 0.21272873878479004, Accuracy: 92.31874084472656, Test Loss: 1.0857930183410645, Test Accuracy: 76.6159439086914\n",
            "Epoch 227, Loss: 0.21187710762023926, Accuracy: 92.34981536865234, Test Loss: 1.0873597860336304, Test Accuracy: 76.62879943847656\n",
            "Epoch 228, Loss: 0.21103626489639282, Accuracy: 92.38172149658203, Test Loss: 1.0885374546051025, Test Accuracy: 76.64409637451172\n",
            "Epoch 229, Loss: 0.21025440096855164, Accuracy: 92.4084243774414, Test Loss: 1.0927401781082153, Test Accuracy: 76.63372039794922\n",
            "Epoch 230, Loss: 0.2099590301513672, Accuracy: 92.42617797851562, Test Loss: 1.0926895141601562, Test Accuracy: 76.64378356933594\n",
            "Epoch 231, Loss: 0.21020004153251648, Accuracy: 92.42044830322266, Test Loss: 1.0923335552215576, Test Accuracy: 76.65375518798828\n",
            "Epoch 232, Loss: 0.20997299253940582, Accuracy: 92.42232513427734, Test Loss: 1.0925294160842896, Test Accuracy: 76.67120361328125\n",
            "Epoch 233, Loss: 0.20929716527462006, Accuracy: 92.44786071777344, Test Loss: 1.0917271375656128, Test Accuracy: 76.69351959228516\n",
            "Epoch 234, Loss: 0.20849870145320892, Accuracy: 92.47691345214844, Test Loss: 1.0921974182128906, Test Accuracy: 76.70315551757812\n",
            "Epoch 235, Loss: 0.2077469676733017, Accuracy: 92.50466918945312, Test Loss: 1.0940794944763184, Test Accuracy: 76.70523834228516\n",
            "Epoch 236, Loss: 0.20695963501930237, Accuracy: 92.5343017578125, Test Loss: 1.093809962272644, Test Accuracy: 76.7370376586914\n",
            "Epoch 237, Loss: 0.20614509284496307, Accuracy: 92.56421661376953, Test Loss: 1.09389328956604, Test Accuracy: 76.76611328125\n",
            "Epoch 238, Loss: 0.2053464949131012, Accuracy: 92.59282684326172, Test Loss: 1.0959781408309937, Test Accuracy: 76.75807189941406\n",
            "Epoch 239, Loss: 0.20454080402851105, Accuracy: 92.623291015625, Test Loss: 1.0968475341796875, Test Accuracy: 76.78191375732422\n",
            "Epoch 240, Loss: 0.20401208102703094, Accuracy: 92.6446304321289, Test Loss: 1.098109483718872, Test Accuracy: 76.78606414794922\n",
            "Epoch 241, Loss: 0.2058725208044052, Accuracy: 92.6033935546875, Test Loss: 1.0973827838897705, Test Accuracy: 76.76591491699219\n",
            "Epoch 242, Loss: 0.2056315392255783, Accuracy: 92.61531829833984, Test Loss: 1.0962905883789062, Test Accuracy: 76.77251434326172\n",
            "Epoch 243, Loss: 0.2053343802690506, Accuracy: 92.62920379638672, Test Loss: 1.0953305959701538, Test Accuracy: 76.77664947509766\n",
            "Epoch 244, Loss: 0.20466910302639008, Accuracy: 92.65376281738281, Test Loss: 1.0951614379882812, Test Accuracy: 76.78553771972656\n",
            "Epoch 245, Loss: 0.20393799245357513, Accuracy: 92.68119049072266, Test Loss: 1.094744086265564, Test Accuracy: 76.79436492919922\n",
            "Epoch 246, Loss: 0.2032541036605835, Accuracy: 92.70687103271484, Test Loss: 1.094620704650879, Test Accuracy: 76.80787658691406\n",
            "Epoch 247, Loss: 0.202494814991951, Accuracy: 92.73588562011719, Test Loss: 1.094091534614563, Test Accuracy: 76.83547973632812\n",
            "Epoch 248, Loss: 0.20178429782390594, Accuracy: 92.76062774658203, Test Loss: 1.0953636169433594, Test Accuracy: 76.83927154541016\n",
            "Epoch 249, Loss: 0.20103110373020172, Accuracy: 92.78819274902344, Test Loss: 1.0962492227554321, Test Accuracy: 76.85009002685547\n",
            "Epoch 250, Loss: 0.20031216740608215, Accuracy: 92.81352996826172, Test Loss: 1.0969797372817993, Test Accuracy: 76.86082458496094\n",
            "Epoch 251, Loss: 0.1996963918209076, Accuracy: 92.836669921875, Test Loss: 1.0974739789962769, Test Accuracy: 76.87844848632812\n",
            "Epoch 252, Loss: 0.19915682077407837, Accuracy: 92.8576431274414, Test Loss: 1.1009562015533447, Test Accuracy: 76.86113739013672\n",
            "Epoch 253, Loss: 0.20046183466911316, Accuracy: 92.83088684082031, Test Loss: 1.099503993988037, Test Accuracy: 76.8624496459961\n",
            "Epoch 254, Loss: 0.20047684013843536, Accuracy: 92.83494567871094, Test Loss: 1.1000874042510986, Test Accuracy: 76.85913848876953\n",
            "Epoch 255, Loss: 0.1999937742948532, Accuracy: 92.85419464111328, Test Loss: 1.099490761756897, Test Accuracy: 76.87879943847656\n",
            "Epoch 256, Loss: 0.19939760863780975, Accuracy: 92.87721252441406, Test Loss: 1.0996856689453125, Test Accuracy: 76.88459014892578\n",
            "Epoch 257, Loss: 0.19889496266841888, Accuracy: 92.89517211914062, Test Loss: 1.0987603664398193, Test Accuracy: 76.9085464477539\n",
            "Epoch 258, Loss: 0.1982879340648651, Accuracy: 92.9159164428711, Test Loss: 1.09870183467865, Test Accuracy: 76.92324829101562\n",
            "Epoch 259, Loss: 0.19770532846450806, Accuracy: 92.93697357177734, Test Loss: 1.0985475778579712, Test Accuracy: 76.94235229492188\n",
            "Epoch 260, Loss: 0.19720697402954102, Accuracy: 92.95450592041016, Test Loss: 1.0995088815689087, Test Accuracy: 76.92982482910156\n",
            "Epoch 261, Loss: 0.19699490070343018, Accuracy: 92.9618148803711, Test Loss: 1.099894404411316, Test Accuracy: 76.93531799316406\n",
            "Epoch 262, Loss: 0.19635799527168274, Accuracy: 92.98484802246094, Test Loss: 1.0999282598495483, Test Accuracy: 76.9429931640625\n",
            "Epoch 263, Loss: 0.1957326978445053, Accuracy: 93.00770568847656, Test Loss: 1.1012275218963623, Test Accuracy: 76.93282318115234\n",
            "Epoch 264, Loss: 0.19509448111057281, Accuracy: 93.0313491821289, Test Loss: 1.1037832498550415, Test Accuracy: 76.92716979980469\n",
            "Epoch 265, Loss: 0.1944916844367981, Accuracy: 93.05244445800781, Test Loss: 1.1053229570388794, Test Accuracy: 76.93037414550781\n",
            "Epoch 266, Loss: 0.1940268874168396, Accuracy: 93.06866455078125, Test Loss: 1.10854172706604, Test Accuracy: 76.92037200927734\n",
            "Epoch 267, Loss: 0.19352297484874725, Accuracy: 93.08759307861328, Test Loss: 1.1104693412780762, Test Accuracy: 76.92577362060547\n",
            "Epoch 268, Loss: 0.1929270476102829, Accuracy: 93.110107421875, Test Loss: 1.114309549331665, Test Accuracy: 76.91368103027344\n",
            "Epoch 269, Loss: 0.19226515293121338, Accuracy: 93.13432312011719, Test Loss: 1.1143769025802612, Test Accuracy: 76.93862915039062\n",
            "Epoch 270, Loss: 0.19160036742687225, Accuracy: 93.15882110595703, Test Loss: 1.115992546081543, Test Accuracy: 76.93307495117188\n",
            "Epoch 271, Loss: 0.19095610082149506, Accuracy: 93.18222045898438, Test Loss: 1.1177525520324707, Test Accuracy: 76.92755889892578\n",
            "Epoch 272, Loss: 0.1903311014175415, Accuracy: 93.20543670654297, Test Loss: 1.119005560874939, Test Accuracy: 76.9371337890625\n",
            "Epoch 273, Loss: 0.18969696760177612, Accuracy: 93.22757720947266, Test Loss: 1.1211930513381958, Test Accuracy: 76.93379211425781\n",
            "Epoch 274, Loss: 0.1891019344329834, Accuracy: 93.25000762939453, Test Loss: 1.1255961656570435, Test Accuracy: 76.92406463623047\n",
            "Epoch 275, Loss: 0.18895301222801208, Accuracy: 93.25723266601562, Test Loss: 1.1301789283752441, Test Accuracy: 76.89739227294922\n",
            "Epoch 276, Loss: 0.18878863751888275, Accuracy: 93.26441192626953, Test Loss: 1.1294466257095337, Test Accuracy: 76.91542053222656\n",
            "Epoch 277, Loss: 0.18831445276737213, Accuracy: 93.28194427490234, Test Loss: 1.1314994096755981, Test Accuracy: 76.9058609008789\n",
            "Epoch 278, Loss: 0.18785251677036285, Accuracy: 93.2970962524414, Test Loss: 1.1319173574447632, Test Accuracy: 76.91531372070312\n",
            "Epoch 279, Loss: 0.18739067018032074, Accuracy: 93.3134765625, Test Loss: 1.1314066648483276, Test Accuracy: 76.93936157226562\n",
            "Epoch 280, Loss: 0.18679684400558472, Accuracy: 93.33467102050781, Test Loss: 1.1311010122299194, Test Accuracy: 76.95697784423828\n",
            "Epoch 281, Loss: 0.1862376481294632, Accuracy: 93.35437774658203, Test Loss: 1.132003903388977, Test Accuracy: 76.95781707763672\n",
            "Epoch 282, Loss: 0.18567758798599243, Accuracy: 93.37394714355469, Test Loss: 1.133625864982605, Test Accuracy: 76.96487426757812\n",
            "Epoch 283, Loss: 0.18512140214443207, Accuracy: 93.39514923095703, Test Loss: 1.1361178159713745, Test Accuracy: 76.96360778808594\n",
            "Epoch 284, Loss: 0.18457482755184174, Accuracy: 93.4144287109375, Test Loss: 1.1380696296691895, Test Accuracy: 76.96853637695312\n",
            "Epoch 285, Loss: 0.18431468307971954, Accuracy: 93.42391204833984, Test Loss: 1.1397219896316528, Test Accuracy: 76.9467544555664\n",
            "Epoch 286, Loss: 0.18477849662303925, Accuracy: 93.41184997558594, Test Loss: 1.141314148902893, Test Accuracy: 76.91285705566406\n",
            "Epoch 287, Loss: 0.18453852832317352, Accuracy: 93.4221420288086, Test Loss: 1.1409944295883179, Test Accuracy: 76.91178894042969\n",
            "Epoch 288, Loss: 0.18416373431682587, Accuracy: 93.43672180175781, Test Loss: 1.1414811611175537, Test Accuracy: 76.90667724609375\n",
            "Epoch 289, Loss: 0.18367421627044678, Accuracy: 93.4542236328125, Test Loss: 1.1427925825119019, Test Accuracy: 76.90564727783203\n",
            "Epoch 290, Loss: 0.18326659500598907, Accuracy: 93.46858215332031, Test Loss: 1.1422525644302368, Test Accuracy: 76.91873168945312\n",
            "Epoch 291, Loss: 0.18282842636108398, Accuracy: 93.48456573486328, Test Loss: 1.1419450044631958, Test Accuracy: 76.93173217773438\n",
            "Epoch 292, Loss: 0.18242986500263214, Accuracy: 93.49958801269531, Test Loss: 1.143497109413147, Test Accuracy: 76.93663024902344\n",
            "Epoch 293, Loss: 0.18200871348381042, Accuracy: 93.51407623291016, Test Loss: 1.1453030109405518, Test Accuracy: 76.92153930664062\n",
            "Epoch 294, Loss: 0.18148311972618103, Accuracy: 93.53273010253906, Test Loss: 1.1449967622756958, Test Accuracy: 76.94036865234375\n",
            "Epoch 295, Loss: 0.18118727207183838, Accuracy: 93.54275512695312, Test Loss: 1.1443123817443848, Test Accuracy: 76.96104431152344\n",
            "Epoch 296, Loss: 0.18073275685310364, Accuracy: 93.55780029296875, Test Loss: 1.1449122428894043, Test Accuracy: 76.96183013916016\n",
            "Epoch 297, Loss: 0.1802242547273636, Accuracy: 93.57569122314453, Test Loss: 1.1466162204742432, Test Accuracy: 76.95867156982422\n",
            "Epoch 298, Loss: 0.17975522577762604, Accuracy: 93.59220123291016, Test Loss: 1.1492618322372437, Test Accuracy: 76.94572448730469\n",
            "Epoch 299, Loss: 0.17922282218933105, Accuracy: 93.6106948852539, Test Loss: 1.1495850086212158, Test Accuracy: 76.95828247070312\n",
            "Epoch 300, Loss: 0.17872533202171326, Accuracy: 93.62907409667969, Test Loss: 1.1513772010803223, Test Accuracy: 76.95516204833984\n",
            "Epoch 301, Loss: 0.17821472883224487, Accuracy: 93.64691162109375, Test Loss: 1.152156114578247, Test Accuracy: 76.96372985839844\n",
            "Epoch 302, Loss: 0.17766280472278595, Accuracy: 93.66670227050781, Test Loss: 1.1523454189300537, Test Accuracy: 76.98578643798828\n",
            "Epoch 303, Loss: 0.17729409039020538, Accuracy: 93.68057250976562, Test Loss: 1.1553499698638916, Test Accuracy: 76.98261260986328\n",
            "Epoch 304, Loss: 0.17695364356040955, Accuracy: 93.693115234375, Test Loss: 1.155398964881897, Test Accuracy: 77.00061798095703\n",
            "Epoch 305, Loss: 0.17695371806621552, Accuracy: 93.69407653808594, Test Loss: 1.1555410623550415, Test Accuracy: 77.00699615478516\n",
            "Epoch 306, Loss: 0.17660720646381378, Accuracy: 93.7060775756836, Test Loss: 1.1577651500701904, Test Accuracy: 76.98849487304688\n",
            "Epoch 307, Loss: 0.1761317253112793, Accuracy: 93.722900390625, Test Loss: 1.1573981046676636, Test Accuracy: 77.00249481201172\n",
            "Epoch 308, Loss: 0.1756589114665985, Accuracy: 93.74003601074219, Test Loss: 1.158443570137024, Test Accuracy: 77.01260375976562\n",
            "Epoch 309, Loss: 0.17530350387096405, Accuracy: 93.75218200683594, Test Loss: 1.1586410999298096, Test Accuracy: 77.02075958251953\n",
            "Epoch 310, Loss: 0.17488762736320496, Accuracy: 93.7674789428711, Test Loss: 1.1603546142578125, Test Accuracy: 77.01565551757812\n",
            "Epoch 311, Loss: 0.17439089715480804, Accuracy: 93.78511047363281, Test Loss: 1.1600966453552246, Test Accuracy: 77.0350341796875\n",
            "Epoch 312, Loss: 0.17393189668655396, Accuracy: 93.80101013183594, Test Loss: 1.1608821153640747, Test Accuracy: 77.0392837524414\n",
            "Epoch 313, Loss: 0.17362508177757263, Accuracy: 93.81280517578125, Test Loss: 1.1625827550888062, Test Accuracy: 77.04725646972656\n",
            "Epoch 314, Loss: 0.17318814992904663, Accuracy: 93.82852172851562, Test Loss: 1.1629424095153809, Test Accuracy: 77.06075286865234\n",
            "Epoch 315, Loss: 0.17275381088256836, Accuracy: 93.84294128417969, Test Loss: 1.1622824668884277, Test Accuracy: 77.0760269165039\n",
            "Epoch 316, Loss: 0.1722673922777176, Accuracy: 93.86004638671875, Test Loss: 1.1626029014587402, Test Accuracy: 77.08934783935547\n",
            "Epoch 317, Loss: 0.17187432944774628, Accuracy: 93.87348175048828, Test Loss: 1.1636604070663452, Test Accuracy: 77.0988998413086\n",
            "Epoch 318, Loss: 0.17141109704971313, Accuracy: 93.88960266113281, Test Loss: 1.1638096570968628, Test Accuracy: 77.1157455444336\n",
            "Epoch 319, Loss: 0.17105169594287872, Accuracy: 93.90364837646484, Test Loss: 1.1649669408798218, Test Accuracy: 77.1178207397461\n",
            "Epoch 320, Loss: 0.17177768051624298, Accuracy: 93.88862609863281, Test Loss: 1.1641043424606323, Test Accuracy: 77.11988067626953\n",
            "Epoch 321, Loss: 0.17193372547626495, Accuracy: 93.88385772705078, Test Loss: 1.163690447807312, Test Accuracy: 77.1201171875\n",
            "Epoch 322, Loss: 0.17155127227306366, Accuracy: 93.89973449707031, Test Loss: 1.1631847620010376, Test Accuracy: 77.12578582763672\n",
            "Epoch 323, Loss: 0.17112040519714355, Accuracy: 93.91590118408203, Test Loss: 1.162203311920166, Test Accuracy: 77.14591217041016\n",
            "Epoch 324, Loss: 0.17074280977249146, Accuracy: 93.9284896850586, Test Loss: 1.1646384000778198, Test Accuracy: 77.12078857421875\n",
            "Epoch 325, Loss: 0.1720333993434906, Accuracy: 93.89627838134766, Test Loss: 1.1638294458389282, Test Accuracy: 77.11201477050781\n",
            "Epoch 326, Loss: 0.1720096319913864, Accuracy: 93.90155029296875, Test Loss: 1.162995457649231, Test Accuracy: 77.11943054199219\n",
            "Epoch 327, Loss: 0.1717108190059662, Accuracy: 93.91214752197266, Test Loss: 1.162139892578125, Test Accuracy: 77.1321792602539\n",
            "Epoch 328, Loss: 0.1713503748178482, Accuracy: 93.92574310302734, Test Loss: 1.1613456010818481, Test Accuracy: 77.14305877685547\n",
            "Epoch 329, Loss: 0.1710154414176941, Accuracy: 93.93696594238281, Test Loss: 1.161514401435852, Test Accuracy: 77.14676666259766\n",
            "Epoch 330, Loss: 0.1708189845085144, Accuracy: 93.94357299804688, Test Loss: 1.1606669425964355, Test Accuracy: 77.16285705566406\n",
            "Epoch 331, Loss: 0.17040354013442993, Accuracy: 93.95997619628906, Test Loss: 1.1600072383880615, Test Accuracy: 77.17001342773438\n",
            "Epoch 332, Loss: 0.1707320660352707, Accuracy: 93.95439147949219, Test Loss: 1.162750244140625, Test Accuracy: 77.1366195678711\n",
            "Epoch 333, Loss: 0.17083755135536194, Accuracy: 93.95184326171875, Test Loss: 1.1618627309799194, Test Accuracy: 77.14381408691406\n",
            "Epoch 334, Loss: 0.17052718997001648, Accuracy: 93.9646987915039, Test Loss: 1.1610631942749023, Test Accuracy: 77.1527099609375\n",
            "Epoch 335, Loss: 0.17018412053585052, Accuracy: 93.97822570800781, Test Loss: 1.160529613494873, Test Accuracy: 77.15806579589844\n",
            "Epoch 336, Loss: 0.169761523604393, Accuracy: 93.99465942382812, Test Loss: 1.1601135730743408, Test Accuracy: 77.16339111328125\n",
            "Epoch 337, Loss: 0.16935476660728455, Accuracy: 94.01024627685547, Test Loss: 1.160607099533081, Test Accuracy: 77.17215728759766\n",
            "Epoch 338, Loss: 0.16894879937171936, Accuracy: 94.0246353149414, Test Loss: 1.160354733467102, Test Accuracy: 77.18260192871094\n",
            "Epoch 339, Loss: 0.1685563176870346, Accuracy: 94.04077911376953, Test Loss: 1.1606695652008057, Test Accuracy: 77.19297790527344\n",
            "Epoch 340, Loss: 0.16820313036441803, Accuracy: 94.05314636230469, Test Loss: 1.1609923839569092, Test Accuracy: 77.19986724853516\n",
            "Epoch 341, Loss: 0.16776767373085022, Accuracy: 94.0680160522461, Test Loss: 1.1616287231445312, Test Accuracy: 77.21013641357422\n",
            "Epoch 342, Loss: 0.1674744337797165, Accuracy: 94.0787582397461, Test Loss: 1.1630737781524658, Test Accuracy: 77.2049560546875\n",
            "Epoch 343, Loss: 0.16716784238815308, Accuracy: 94.08944702148438, Test Loss: 1.1626993417739868, Test Accuracy: 77.22367095947266\n",
            "Epoch 344, Loss: 0.16680501401424408, Accuracy: 94.10079956054688, Test Loss: 1.1628576517105103, Test Accuracy: 77.2320785522461\n",
            "Epoch 345, Loss: 0.16642628610134125, Accuracy: 94.11463928222656, Test Loss: 1.1637221574783325, Test Accuracy: 77.23535919189453\n",
            "Epoch 346, Loss: 0.16603615880012512, Accuracy: 94.12874603271484, Test Loss: 1.1645017862319946, Test Accuracy: 77.2335433959961\n",
            "Epoch 347, Loss: 0.16582714021205902, Accuracy: 94.13735961914062, Test Loss: 1.1641323566436768, Test Accuracy: 77.24185943603516\n",
            "Epoch 348, Loss: 0.1655777543783188, Accuracy: 94.14700317382812, Test Loss: 1.1646854877471924, Test Accuracy: 77.2484359741211\n",
            "Epoch 349, Loss: 0.1652207374572754, Accuracy: 94.15982055664062, Test Loss: 1.164719581604004, Test Accuracy: 77.25833129882812\n",
            "Epoch 350, Loss: 0.16497157514095306, Accuracy: 94.16827392578125, Test Loss: 1.1655720472335815, Test Accuracy: 77.25814819335938\n",
            "Epoch 351, Loss: 0.16467832028865814, Accuracy: 94.17882537841797, Test Loss: 1.1675246953964233, Test Accuracy: 77.25463104248047\n",
            "Epoch 352, Loss: 0.1643727868795395, Accuracy: 94.18965911865234, Test Loss: 1.1676732301712036, Test Accuracy: 77.26441955566406\n",
            "Epoch 353, Loss: 0.16403059661388397, Accuracy: 94.20150756835938, Test Loss: 1.16732919216156, Test Accuracy: 77.2708511352539\n",
            "Epoch 354, Loss: 0.16363182663917542, Accuracy: 94.21575927734375, Test Loss: 1.167332410812378, Test Accuracy: 77.28218841552734\n",
            "Epoch 355, Loss: 0.16329391300678253, Accuracy: 94.22817993164062, Test Loss: 1.1671106815338135, Test Accuracy: 77.29676055908203\n",
            "Epoch 356, Loss: 0.1629253327846527, Accuracy: 94.24156951904297, Test Loss: 1.1675021648406982, Test Accuracy: 77.30139923095703\n",
            "Epoch 357, Loss: 0.16261102259159088, Accuracy: 94.25243377685547, Test Loss: 1.1686019897460938, Test Accuracy: 77.29782104492188\n",
            "Epoch 358, Loss: 0.16247917711734772, Accuracy: 94.25799560546875, Test Loss: 1.1696840524673462, Test Accuracy: 77.29752349853516\n",
            "Epoch 359, Loss: 0.16220127046108246, Accuracy: 94.26805114746094, Test Loss: 1.1695525646209717, Test Accuracy: 77.3070068359375\n",
            "Epoch 360, Loss: 0.1619880050420761, Accuracy: 94.27735900878906, Test Loss: 1.169455647468567, Test Accuracy: 77.31806945800781\n",
            "Epoch 361, Loss: 0.16220493614673615, Accuracy: 94.27238464355469, Test Loss: 1.171319842338562, Test Accuracy: 77.2966537475586\n",
            "Epoch 362, Loss: 0.16211551427841187, Accuracy: 94.27574157714844, Test Loss: 1.1708765029907227, Test Accuracy: 77.30606079101562\n",
            "Epoch 363, Loss: 0.16176831722259521, Accuracy: 94.28840637207031, Test Loss: 1.1702934503555298, Test Accuracy: 77.32025146484375\n",
            "Epoch 364, Loss: 0.16146202385425568, Accuracy: 94.29996490478516, Test Loss: 1.1696045398712158, Test Accuracy: 77.33114624023438\n",
            "Epoch 365, Loss: 0.16111940145492554, Accuracy: 94.31181335449219, Test Loss: 1.168784499168396, Test Accuracy: 77.34358978271484\n",
            "Epoch 366, Loss: 0.16082434356212616, Accuracy: 94.32152557373047, Test Loss: 1.168010950088501, Test Accuracy: 77.35436248779297\n",
            "Epoch 367, Loss: 0.16049309074878693, Accuracy: 94.33256530761719, Test Loss: 1.167649269104004, Test Accuracy: 77.36188507080078\n",
            "Epoch 368, Loss: 0.16017524898052216, Accuracy: 94.34284973144531, Test Loss: 1.16744065284729, Test Accuracy: 77.36937713623047\n",
            "Epoch 369, Loss: 0.15984977781772614, Accuracy: 94.35343170166016, Test Loss: 1.1669780015945435, Test Accuracy: 77.38316345214844\n",
            "Epoch 370, Loss: 0.15963450074195862, Accuracy: 94.35920715332031, Test Loss: 1.1669402122497559, Test Accuracy: 77.39054870605469\n",
            "Epoch 371, Loss: 0.1593250036239624, Accuracy: 94.36934661865234, Test Loss: 1.1669074296951294, Test Accuracy: 77.40104675292969\n",
            "Epoch 372, Loss: 0.15898540616035461, Accuracy: 94.38212585449219, Test Loss: 1.1671173572540283, Test Accuracy: 77.40991973876953\n",
            "Epoch 373, Loss: 0.15865632891654968, Accuracy: 94.39450073242188, Test Loss: 1.166670322418213, Test Accuracy: 77.42501831054688\n",
            "Epoch 374, Loss: 0.1583815962076187, Accuracy: 94.40278625488281, Test Loss: 1.1669052839279175, Test Accuracy: 77.43221282958984\n",
            "Epoch 375, Loss: 0.1580953150987625, Accuracy: 94.41303253173828, Test Loss: 1.1670242547988892, Test Accuracy: 77.43470001220703\n",
            "Epoch 376, Loss: 0.15776292979717255, Accuracy: 94.42422485351562, Test Loss: 1.16725754737854, Test Accuracy: 77.44027709960938\n",
            "Epoch 377, Loss: 0.15749859809875488, Accuracy: 94.43270111083984, Test Loss: 1.1681700944900513, Test Accuracy: 77.4458236694336\n",
            "Epoch 378, Loss: 0.15725673735141754, Accuracy: 94.44145965576172, Test Loss: 1.167725920677185, Test Accuracy: 77.46217346191406\n",
            "Epoch 379, Loss: 0.15720508992671967, Accuracy: 94.44356536865234, Test Loss: 1.1675422191619873, Test Accuracy: 77.46763610839844\n",
            "Epoch 380, Loss: 0.15701457858085632, Accuracy: 94.45126342773438, Test Loss: 1.1674333810806274, Test Accuracy: 77.47306823730469\n",
            "Epoch 381, Loss: 0.15676996111869812, Accuracy: 94.45858764648438, Test Loss: 1.1671690940856934, Test Accuracy: 77.48307800292969\n",
            "Epoch 382, Loss: 0.1565769612789154, Accuracy: 94.46521759033203, Test Loss: 1.166865587234497, Test Accuracy: 77.48997497558594\n",
            "Epoch 383, Loss: 0.15632334351539612, Accuracy: 94.47378540039062, Test Loss: 1.1666746139526367, Test Accuracy: 77.49683380126953\n",
            "Epoch 384, Loss: 0.15621191263198853, Accuracy: 94.4770736694336, Test Loss: 1.1664626598358154, Test Accuracy: 77.49756622314453\n",
            "Epoch 385, Loss: 0.15593643486499786, Accuracy: 94.48556518554688, Test Loss: 1.1658309698104858, Test Accuracy: 77.49980926513672\n",
            "Epoch 386, Loss: 0.15565995872020721, Accuracy: 94.49530792236328, Test Loss: 1.1656665802001953, Test Accuracy: 77.506591796875\n",
            "Epoch 387, Loss: 0.1554384082555771, Accuracy: 94.50241088867188, Test Loss: 1.1653506755828857, Test Accuracy: 77.51031494140625\n",
            "Epoch 388, Loss: 0.15514524281024933, Accuracy: 94.51270294189453, Test Loss: 1.1648714542388916, Test Accuracy: 77.52155303955078\n",
            "Epoch 389, Loss: 0.15482951700687408, Accuracy: 94.52326202392578, Test Loss: 1.1642258167266846, Test Accuracy: 77.5312271118164\n",
            "Epoch 390, Loss: 0.15446171164512634, Accuracy: 94.53665924072266, Test Loss: 1.1638792753219604, Test Accuracy: 77.5423583984375\n",
            "Epoch 391, Loss: 0.15407627820968628, Accuracy: 94.55030822753906, Test Loss: 1.1634533405303955, Test Accuracy: 77.5504379272461\n",
            "Epoch 392, Loss: 0.1536884605884552, Accuracy: 94.56421661376953, Test Loss: 1.1632863283157349, Test Accuracy: 77.55997467041016\n",
            "Epoch 393, Loss: 0.15329980850219727, Accuracy: 94.57804870605469, Test Loss: 1.1630216836929321, Test Accuracy: 77.57093811035156\n",
            "Epoch 394, Loss: 0.15291276574134827, Accuracy: 94.59181213378906, Test Loss: 1.162777066230774, Test Accuracy: 77.58185577392578\n",
            "Epoch 395, Loss: 0.15252751111984253, Accuracy: 94.60549926757812, Test Loss: 1.1625564098358154, Test Accuracy: 77.59420013427734\n",
            "Epoch 396, Loss: 0.15214404463768005, Accuracy: 94.61912536621094, Test Loss: 1.162356972694397, Test Accuracy: 77.60647583007812\n",
            "Epoch 397, Loss: 0.15176239609718323, Accuracy: 94.63268280029297, Test Loss: 1.1621801853179932, Test Accuracy: 77.61869049072266\n",
            "Epoch 398, Loss: 0.15138255059719086, Accuracy: 94.64616394042969, Test Loss: 1.1620228290557861, Test Accuracy: 77.63084411621094\n",
            "Epoch 399, Loss: 0.15100453794002533, Accuracy: 94.65957641601562, Test Loss: 1.1618860960006714, Test Accuracy: 77.64293670654297\n",
            "Epoch 400, Loss: 0.15062832832336426, Accuracy: 94.67292785644531, Test Loss: 1.1617685556411743, Test Accuracy: 77.65496826171875\n",
            "Epoch 401, Loss: 0.15025390684604645, Accuracy: 94.68621826171875, Test Loss: 1.161670446395874, Test Accuracy: 77.66694641113281\n",
            "Epoch 402, Loss: 0.1498812735080719, Accuracy: 94.6994400024414, Test Loss: 1.1615885496139526, Test Accuracy: 77.6788558959961\n",
            "Epoch 403, Loss: 0.14951041340827942, Accuracy: 94.71258544921875, Test Loss: 1.1615266799926758, Test Accuracy: 77.69071197509766\n",
            "Epoch 404, Loss: 0.1491413563489914, Accuracy: 94.72567749023438, Test Loss: 1.1614826917648315, Test Accuracy: 77.70250701904297\n",
            "Epoch 405, Loss: 0.14877407252788544, Accuracy: 94.73870086669922, Test Loss: 1.161454200744629, Test Accuracy: 77.71424865722656\n",
            "Epoch 406, Loss: 0.14840856194496155, Accuracy: 94.75165557861328, Test Loss: 1.161441683769226, Test Accuracy: 77.72592163085938\n",
            "Epoch 407, Loss: 0.14804480969905853, Accuracy: 94.76455688476562, Test Loss: 1.161443829536438, Test Accuracy: 77.737548828125\n",
            "Epoch 408, Loss: 0.14768283069133759, Accuracy: 94.77738952636719, Test Loss: 1.161460280418396, Test Accuracy: 77.74910736083984\n",
            "Epoch 409, Loss: 0.14732256531715393, Accuracy: 94.79015350341797, Test Loss: 1.1614900827407837, Test Accuracy: 77.7606201171875\n",
            "Epoch 410, Loss: 0.146963968873024, Accuracy: 94.80286407470703, Test Loss: 1.1615333557128906, Test Accuracy: 77.7720718383789\n",
            "Epoch 411, Loss: 0.14660713076591492, Accuracy: 94.81550598144531, Test Loss: 1.161590337753296, Test Accuracy: 77.7834701538086\n",
            "Epoch 412, Loss: 0.14625202119350433, Accuracy: 94.82808685302734, Test Loss: 1.1616628170013428, Test Accuracy: 77.79480743408203\n",
            "Epoch 413, Loss: 0.14589858055114746, Accuracy: 94.84060668945312, Test Loss: 1.1617462635040283, Test Accuracy: 77.80609893798828\n",
            "Epoch 414, Loss: 0.1455468088388443, Accuracy: 94.85307312011719, Test Loss: 1.1618455648422241, Test Accuracy: 77.81732940673828\n",
            "Epoch 415, Loss: 0.14519670605659485, Accuracy: 94.865478515625, Test Loss: 1.1619548797607422, Test Accuracy: 77.82850646972656\n",
            "Epoch 416, Loss: 0.14484825730323792, Accuracy: 94.87782287597656, Test Loss: 1.1620807647705078, Test Accuracy: 77.83963012695312\n",
            "Epoch 417, Loss: 0.1445014625787735, Accuracy: 94.89010620117188, Test Loss: 1.1622157096862793, Test Accuracy: 77.85070037841797\n",
            "Epoch 418, Loss: 0.1441563218832016, Accuracy: 94.90232849121094, Test Loss: 1.1623671054840088, Test Accuracy: 77.8617172241211\n",
            "Epoch 419, Loss: 0.14381280541419983, Accuracy: 94.91449737548828, Test Loss: 1.1625291109085083, Test Accuracy: 77.87268829345703\n",
            "Epoch 420, Loss: 0.1434708833694458, Accuracy: 94.92660522460938, Test Loss: 1.1627041101455688, Test Accuracy: 77.88359832763672\n",
            "Epoch 421, Loss: 0.1431305706501007, Accuracy: 94.93865203857422, Test Loss: 1.1628899574279785, Test Accuracy: 77.89446258544922\n",
            "Epoch 422, Loss: 0.14279182255268097, Accuracy: 94.95064544677734, Test Loss: 1.1630890369415283, Test Accuracy: 77.90526580810547\n",
            "Epoch 423, Loss: 0.14245468378067017, Accuracy: 94.96258544921875, Test Loss: 1.1633002758026123, Test Accuracy: 77.91602325439453\n",
            "Epoch 424, Loss: 0.1421191245317459, Accuracy: 94.9744644165039, Test Loss: 1.163533091545105, Test Accuracy: 77.9267349243164\n",
            "Epoch 425, Loss: 0.14178510010242462, Accuracy: 94.98628997802734, Test Loss: 1.163794994354248, Test Accuracy: 77.93739318847656\n",
            "Epoch 426, Loss: 0.1414525955915451, Accuracy: 94.99805450439453, Test Loss: 1.1641181707382202, Test Accuracy: 77.947998046875\n",
            "Epoch 427, Loss: 0.14112161099910736, Accuracy: 95.00977325439453, Test Loss: 1.1645679473876953, Test Accuracy: 77.95855712890625\n",
            "Epoch 428, Loss: 0.1407920867204666, Accuracy: 95.02143096923828, Test Loss: 1.1651678085327148, Test Accuracy: 77.96907043457031\n",
            "Epoch 429, Loss: 0.14046406745910645, Accuracy: 95.03303527832031, Test Loss: 1.1658719778060913, Test Accuracy: 77.97952270507812\n",
            "Epoch 430, Loss: 0.14013755321502686, Accuracy: 95.04458618164062, Test Loss: 1.1666592359542847, Test Accuracy: 77.98993682861328\n",
            "Epoch 431, Loss: 0.13981249928474426, Accuracy: 95.05608367919922, Test Loss: 1.1675279140472412, Test Accuracy: 78.00029754638672\n",
            "Epoch 432, Loss: 0.13948892056941986, Accuracy: 95.0675277709961, Test Loss: 1.1684677600860596, Test Accuracy: 78.01061248779297\n",
            "Epoch 433, Loss: 0.13916683197021484, Accuracy: 95.07891845703125, Test Loss: 1.1694746017456055, Test Accuracy: 78.02088165283203\n",
            "Epoch 434, Loss: 0.1388462334871292, Accuracy: 95.09025573730469, Test Loss: 1.1705420017242432, Test Accuracy: 78.0324478149414\n",
            "Epoch 435, Loss: 0.13852708041667938, Accuracy: 95.10154724121094, Test Loss: 1.1716643571853638, Test Accuracy: 78.04396057128906\n",
            "Epoch 436, Loss: 0.13820938766002655, Accuracy: 95.11278533935547, Test Loss: 1.1728376150131226, Test Accuracy: 78.055419921875\n",
            "Epoch 437, Loss: 0.13789314031600952, Accuracy: 95.12396240234375, Test Loss: 1.1740604639053345, Test Accuracy: 78.06683349609375\n",
            "Epoch 438, Loss: 0.1375783234834671, Accuracy: 95.13510131835938, Test Loss: 1.1753296852111816, Test Accuracy: 78.07818603515625\n",
            "Epoch 439, Loss: 0.1372649371623993, Accuracy: 95.14617919921875, Test Loss: 1.1766413450241089, Test Accuracy: 78.08949279785156\n",
            "Epoch 440, Loss: 0.1369529664516449, Accuracy: 95.15721130371094, Test Loss: 1.1779921054840088, Test Accuracy: 78.10074615478516\n",
            "Epoch 441, Loss: 0.13664241135120392, Accuracy: 95.1681900024414, Test Loss: 1.179378628730774, Test Accuracy: 78.11194610595703\n",
            "Epoch 442, Loss: 0.13633327186107635, Accuracy: 95.17912292480469, Test Loss: 1.1807982921600342, Test Accuracy: 78.12310028076172\n",
            "Epoch 443, Loss: 0.13602551817893982, Accuracy: 95.19001007080078, Test Loss: 1.182248830795288, Test Accuracy: 78.13419342041016\n",
            "Epoch 444, Loss: 0.1357191652059555, Accuracy: 95.20084381103516, Test Loss: 1.1837326288223267, Test Accuracy: 78.14524841308594\n",
            "Epoch 445, Loss: 0.13541416823863983, Accuracy: 95.21162414550781, Test Loss: 1.1852483749389648, Test Accuracy: 78.15625\n",
            "Epoch 446, Loss: 0.13511055707931519, Accuracy: 95.22235870361328, Test Loss: 1.1867924928665161, Test Accuracy: 78.16720581054688\n",
            "Epoch 447, Loss: 0.13480828702449799, Accuracy: 95.23304748535156, Test Loss: 1.1883639097213745, Test Accuracy: 78.17810821533203\n",
            "Epoch 448, Loss: 0.13450738787651062, Accuracy: 95.24369049072266, Test Loss: 1.1899641752243042, Test Accuracy: 78.18896484375\n",
            "Epoch 449, Loss: 0.1342078149318695, Accuracy: 95.25428771972656, Test Loss: 1.1915916204452515, Test Accuracy: 78.19976806640625\n",
            "Epoch 450, Loss: 0.13390956819057465, Accuracy: 95.26483154296875, Test Loss: 1.1932462453842163, Test Accuracy: 78.21052551269531\n",
            "Epoch 451, Loss: 0.13361264765262604, Accuracy: 95.27532958984375, Test Loss: 1.1949257850646973, Test Accuracy: 78.22123718261719\n",
            "Epoch 452, Loss: 0.13331705331802368, Accuracy: 95.28578186035156, Test Loss: 1.196628212928772, Test Accuracy: 78.23190307617188\n",
            "Epoch 453, Loss: 0.1330227553844452, Accuracy: 95.29618835449219, Test Loss: 1.1983520984649658, Test Accuracy: 78.24251556396484\n",
            "Epoch 454, Loss: 0.13272975385189056, Accuracy: 95.30654907226562, Test Loss: 1.2000983953475952, Test Accuracy: 78.25308227539062\n",
            "Epoch 455, Loss: 0.1324380338191986, Accuracy: 95.31686401367188, Test Loss: 1.2018672227859497, Test Accuracy: 78.26361083984375\n",
            "Epoch 456, Loss: 0.13214759528636932, Accuracy: 95.32713317871094, Test Loss: 1.2036556005477905, Test Accuracy: 78.27408599853516\n",
            "Epoch 457, Loss: 0.1318584382534027, Accuracy: 95.33736419677734, Test Loss: 1.20546293258667, Test Accuracy: 78.28451538085938\n",
            "Epoch 458, Loss: 0.13157053291797638, Accuracy: 95.34754180908203, Test Loss: 1.2072879076004028, Test Accuracy: 78.29490661621094\n",
            "Epoch 459, Loss: 0.13128389418125153, Accuracy: 95.35767364501953, Test Loss: 1.209129810333252, Test Accuracy: 78.30523681640625\n",
            "Epoch 460, Loss: 0.13099849224090576, Accuracy: 95.36776733398438, Test Loss: 1.2109867334365845, Test Accuracy: 78.31553649902344\n",
            "Epoch 461, Loss: 0.1307143270969391, Accuracy: 95.37781524658203, Test Loss: 1.2128554582595825, Test Accuracy: 78.3257827758789\n",
            "Epoch 462, Loss: 0.1304313987493515, Accuracy: 95.38782501220703, Test Loss: 1.2147350311279297, Test Accuracy: 78.33599090576172\n",
            "Epoch 463, Loss: 0.1301496922969818, Accuracy: 95.39778137207031, Test Loss: 1.2166242599487305, Test Accuracy: 78.34615325927734\n",
            "Epoch 464, Loss: 0.12986919283866882, Accuracy: 95.40769958496094, Test Loss: 1.2185221910476685, Test Accuracy: 78.35626983642578\n",
            "Epoch 465, Loss: 0.12958990037441254, Accuracy: 95.4175796508789, Test Loss: 1.2204278707504272, Test Accuracy: 78.36634826660156\n",
            "Epoch 466, Loss: 0.12931181490421295, Accuracy: 95.42741394042969, Test Loss: 1.2223414182662964, Test Accuracy: 78.37637329101562\n",
            "Epoch 467, Loss: 0.12903492152690887, Accuracy: 95.43720245361328, Test Loss: 1.2242621183395386, Test Accuracy: 78.38636779785156\n",
            "Epoch 468, Loss: 0.1287592053413391, Accuracy: 95.44695281982422, Test Loss: 1.2261875867843628, Test Accuracy: 78.39630889892578\n",
            "Epoch 469, Loss: 0.12848466634750366, Accuracy: 95.45665740966797, Test Loss: 1.2281173467636108, Test Accuracy: 78.40621185302734\n",
            "Epoch 470, Loss: 0.12821128964424133, Accuracy: 95.46632385253906, Test Loss: 1.2300517559051514, Test Accuracy: 78.41607666015625\n",
            "Epoch 471, Loss: 0.12793907523155212, Accuracy: 95.4759521484375, Test Loss: 1.231989860534668, Test Accuracy: 78.42589569091797\n",
            "Epoch 472, Loss: 0.12766802310943604, Accuracy: 95.48553466796875, Test Loss: 1.2339314222335815, Test Accuracy: 78.4356689453125\n",
            "Epoch 473, Loss: 0.12739811837673187, Accuracy: 95.49507904052734, Test Loss: 1.235876202583313, Test Accuracy: 78.4454116821289\n",
            "Epoch 474, Loss: 0.12712934613227844, Accuracy: 95.50458526611328, Test Loss: 1.237824559211731, Test Accuracy: 78.4551010131836\n",
            "Epoch 475, Loss: 0.12686169147491455, Accuracy: 95.51404571533203, Test Loss: 1.2397773265838623, Test Accuracy: 78.46475982666016\n",
            "Epoch 476, Loss: 0.12659518420696259, Accuracy: 95.52346801757812, Test Loss: 1.2417327165603638, Test Accuracy: 78.47437286376953\n",
            "Epoch 477, Loss: 0.12632977962493896, Accuracy: 95.5328598022461, Test Loss: 1.2436909675598145, Test Accuracy: 78.48394775390625\n",
            "Epoch 478, Loss: 0.12606549263000488, Accuracy: 95.54220581054688, Test Loss: 1.245652675628662, Test Accuracy: 78.49348449707031\n",
            "Epoch 479, Loss: 0.12580230832099915, Accuracy: 95.55150604248047, Test Loss: 1.2476164102554321, Test Accuracy: 78.50296783447266\n",
            "Epoch 480, Loss: 0.12554022669792175, Accuracy: 95.56077575683594, Test Loss: 1.249583125114441, Test Accuracy: 78.51242065429688\n",
            "Epoch 481, Loss: 0.12527921795845032, Accuracy: 95.57000732421875, Test Loss: 1.2515504360198975, Test Accuracy: 78.52184295654297\n",
            "Epoch 482, Loss: 0.12501931190490723, Accuracy: 95.57919311523438, Test Loss: 1.2535202503204346, Test Accuracy: 78.53121948242188\n",
            "Epoch 483, Loss: 0.1247604712843895, Accuracy: 95.58834838867188, Test Loss: 1.2554914951324463, Test Accuracy: 78.5405502319336\n",
            "Epoch 484, Loss: 0.12450269609689713, Accuracy: 95.59746551513672, Test Loss: 1.257463812828064, Test Accuracy: 78.54985046386719\n",
            "Epoch 485, Loss: 0.12424599379301071, Accuracy: 95.6065444946289, Test Loss: 1.2594388723373413, Test Accuracy: 78.55911254882812\n",
            "Epoch 486, Loss: 0.12399034202098846, Accuracy: 95.61558532714844, Test Loss: 1.261415958404541, Test Accuracy: 78.5683364868164\n",
            "Epoch 487, Loss: 0.12373574078083038, Accuracy: 95.62458038330078, Test Loss: 1.2633945941925049, Test Accuracy: 78.5775146484375\n",
            "Epoch 488, Loss: 0.12348218262195587, Accuracy: 95.633544921875, Test Loss: 1.265375018119812, Test Accuracy: 78.58666229248047\n",
            "Epoch 489, Loss: 0.12322966754436493, Accuracy: 95.64248657226562, Test Loss: 1.2673580646514893, Test Accuracy: 78.59577178955078\n",
            "Epoch 490, Loss: 0.12297817319631577, Accuracy: 95.65137481689453, Test Loss: 1.2693425416946411, Test Accuracy: 78.60485076904297\n",
            "Epoch 491, Loss: 0.12272771447896957, Accuracy: 95.66022491455078, Test Loss: 1.2713286876678467, Test Accuracy: 78.61387634277344\n",
            "Epoch 492, Loss: 0.12247826904058456, Accuracy: 95.66905212402344, Test Loss: 1.2733162641525269, Test Accuracy: 78.62287902832031\n",
            "Epoch 493, Loss: 0.12222982943058014, Accuracy: 95.6778335571289, Test Loss: 1.2753047943115234, Test Accuracy: 78.63184356689453\n",
            "Epoch 494, Loss: 0.1219824030995369, Accuracy: 95.68658447265625, Test Loss: 1.2772945165634155, Test Accuracy: 78.64076232910156\n",
            "Epoch 495, Loss: 0.12173597514629364, Accuracy: 95.69529724121094, Test Loss: 1.2792853116989136, Test Accuracy: 78.64965057373047\n",
            "Epoch 496, Loss: 0.12149053812026978, Accuracy: 95.7039794921875, Test Loss: 1.2812764644622803, Test Accuracy: 78.65850830078125\n",
            "Epoch 497, Loss: 0.1212460920214653, Accuracy: 95.7126235961914, Test Loss: 1.283267617225647, Test Accuracy: 78.66732788085938\n",
            "Epoch 498, Loss: 0.12100262194871902, Accuracy: 95.72122955322266, Test Loss: 1.2852576971054077, Test Accuracy: 78.67610931396484\n",
            "Epoch 499, Loss: 0.12076013535261154, Accuracy: 95.72980499267578, Test Loss: 1.2872493267059326, Test Accuracy: 78.68485260009766\n",
            "Epoch 500, Loss: 0.12051860988140106, Accuracy: 95.73834228515625, Test Loss: 1.289241075515747, Test Accuracy: 78.69356536865234\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ebdTcRzQ3LsC",
        "colab_type": "text"
      },
      "source": [
        "After this training we can tell that the model is overfitting, so possibles solutions to that would be:\n",
        "\n",
        "* Use more data for training, which might mean augment existing dataset thru transformations.\n",
        "* Add batch normalization.\n",
        "* Add regularization or tune dropout layers, even add more.\n",
        "* Maybe try with convolutional architecture."
      ]
    }
  ]
}